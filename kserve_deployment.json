{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Fine-tuned BLIP Model with KServe\n",
    "\n",
    "This notebook demonstrates how to deploy the fine-tuned BLIP model for Mercedes car image captioning using KServe on a Kubernetes cluster. The deployment will allow for scalable and monitored inference of image captions.\n",
    "\n",
    "## Overview\n",
    "1. Install and import required packages\n",
    "2. Connect to MLflow to access the fine-tuned model\n",
    "3. Create a KServe inference service configuration\n",
    "4. Deploy the model on KServe\n",
    "5. Test the deployed model with sample images\n",
    "6. Monitor the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --quiet --no-cache-dir kserve kubernetes mlflow boto3 torch torchvision transformers pillow matplotlib\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import torch\n",
    "import boto3\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from transformers import BlipProcessor\n",
    "\n",
    "# KServe imports\n",
    "import kserve\n",
    "from kubernetes import client as k8s_client\n",
    "from kubernetes import config as k8s_config\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "print(f\"Using device: {device} with dtype: {model_dtype}\")\n",
    "\n",
    "# Set the namespace for KServe deployment\n",
    "NAMESPACE = \"models-serving\"\n",
    "MODEL_NAME = \"blip-mercedes-captioning\"\n",
    "\n",
    "# Load Kubernetes configuration\n",
    "try:\n",
    "    k8s_config.load_incluster_config()\n",
    "    print(\"Running inside Kubernetes cluster\")\n",
    "except:\n",
    "    try:\n",
    "        k8s_config.load_kube_config()\n",
    "        print(\"Running outside Kubernetes cluster, using local kubeconfig\")\n",
    "    except:\n",
    "        print(\"Cannot find Kubernetes configuration, some operations may fail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh token if needed\n",
    "%update_token\n",
    "\n",
    "# Connect to MLflow to access the fine-tuned model\n",
    "client = MlflowClient()\n",
    "registered_model_name = \"blip_ft_production\"\n",
    "\n",
    "# Get the latest version in production\n",
    "latest_versions = client.get_latest_versions(registered_model_name)\n",
    "latest_version = None\n",
    "\n",
    "for version in latest_versions:\n",
    "    if version.current_stage == \"Production\":\n",
    "        latest_version = version\n",
    "        break\n",
    "    \n",
    "if latest_version is None and len(latest_versions) > 0:\n",
    "    latest_version = latest_versions[0]\n",
    "\n",
    "if latest_version is None:\n",
    "    raise ValueError(f\"No versions found for model {registered_model_name}\")\n",
    "\n",
    "print(f\"\\nUsing fine-tuned BLIP model version: {latest_version.version}, stage: {latest_version.current_stage}\")\n",
    "\n",
    "# Get the model's artifact URI\n",
    "model_uri = f\"models:/{registered_model_name}/{latest_version.version}\"\n",
    "print(f\"Fine-tuned BLIP model URI: {model_uri}\")\n",
    "\n",
    "# Create a local directory to save the model\n",
    "model_dir = f\"{MODEL_NAME}-artifacts\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Download the model from MLflow\n",
    "local_model_path = mlflow.artifacts.download_artifacts(\n",
    "    artifact_uri=mlflow.get_run(latest_version.run_id).info.artifact_uri,\n",
    "    dst_path=model_dir\n",
    ")\n",
    "\n",
    "print(f\"Downloaded model artifacts to: {local_model_path}\")\n",
    "\n",
    "# Check if we need to download the OPT model for generation\n",
    "# Note: This depends on whether your fine-tuned model uses OPT for text generation\n",
    "# If it does, uncomment the following lines\n",
    "'''\n",
    "from transformers import OPTForCausalLM, GPT2Tokenizer\n",
    "opt_model_name = \"facebook/opt-350m\"  # Choose appropriate size based on requirements\n",
    "print(f\"Downloading OPT model: {opt_model_name}\")\n",
    "opt_model = OPTForCausalLM.from_pretrained(opt_model_name)\n",
    "opt_tokenizer = GPT2Tokenizer.from_pretrained(opt_model_name)\n",
    "opt_dir = os.path.join(model_dir, \"opt_model\")\n",
    "os.makedirs(opt_dir, exist_ok=True)\n",
    "opt_model.save_pretrained(opt_dir)\n",
    "opt_tokenizer.save_pretrained(opt_dir)\n",
    "print(f\"Saved OPT model to: {opt_dir}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Custom BLIP Predictor for KServe\n",
    "\n",
    "Now, we'll create a custom predictor class that KServe will use to serve our model. This class will handle:\n",
    "1. Loading the fine-tuned BLIP model\n",
    "2. Processing input images\n",
    "3. Generating captions using the model\n",
    "4. Formatting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom predictor class for the BLIP model\n",
    "# This will be used to create a model.py file for the KServe deployment\n",
    "\n",
    "blip_predictor_code = \"\"\"\n",
    "import os\n",
    "import base64\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import Dict\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# Optional: Import OPT model if needed for generation\n",
    "# from transformers import OPTForCausalLM, GPT2Tokenizer\n",
    "\n",
    "from kserve import Model, ModelServer, model_server\n",
    "\n",
    "class BLIPPredictor(Model):\n",
    "    def __init__(self, name: str, predictor_host: str):\n",
    "        super().__init__(name)\n",
    "        self.predictor_host = predictor_host\n",
    "        self.ready = False\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "        self.base_model_name = \"Salesforce/blip-image-captioning-base\"\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        # Optional: For OPT model\n",
    "        # self.opt_model = None\n",
    "        # self.opt_tokenizer = None\n",
    "        \n",
    "        # Configure image preprocessing\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), \n",
    "                                (0.26862954, 0.26130258, 0.27577711))\n",
    "        ])\n",
    "        \n",
    "        # Default prompt for captions\n",
    "        self.default_prompt = \"This image shows\"\n",
    "    \n",
    "    def load(self):\n",
    "        logging.info(f\"Loading BLIP model on {self.device} with {self.model_dtype}\")\n",
    "        \n",
    "        # Load processor from HuggingFace\n",
    "        self.processor = BlipProcessor.from_pretrained(self.base_model_name)\n",
    "        \n",
    "        # Check if we have a local model or need to load from HuggingFace\n",
    "        local_model_path = \"/mnt/models/model\"\n",
    "        if os.path.exists(local_model_path):\n",
    "            logging.info(f\"Loading fine-tuned model from {local_model_path}\")\n",
    "            self.model = BlipForConditionalGeneration.from_pretrained(\n",
    "                local_model_path,\n",
    "                torch_dtype=self.model_dtype\n",
    "            ).to(self.device)\n",
    "        else:\n",
    "            logging.warning(f\"No fine-tuned model found, loading base model from HuggingFace\")\n",
    "            self.model = BlipForConditionalGeneration.from_pretrained(\n",
    "                self.base_model_name,\n",
    "                torch_dtype=self.model_dtype\n",
    "            ).to(self.device)\n",
    "        \n",
    "        # Optional: Load OPT model for text generation if needed\n",
    "        \"\"\"\n",
    "        opt_model_path = \"/mnt/models/opt_model\"\n",
    "        if os.path.exists(opt_model_path):\n",
    "            logging.info(f\"Loading OPT model from {opt_model_path}\")\n",
    "            self.opt_model = OPTForCausalLM.from_pretrained(opt_model_path).to(self.device)\n",
    "            self.opt_tokenizer = GPT2Tokenizer.from_pretrained(opt_model_path)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model.eval()\n",
    "        self.ready = True\n",
    "        logging.info(\"BLIP model loaded and ready for inference\")\n",
    "        return self.ready\n",
    "    \n",
    "    def preprocess_image(self, image_data):\n",
    "        \"\"\"Process base64 encoded image data\"\"\"\n",
    "        # Decode base64 image\n",
    "        image_bytes = base64.b64decode(image_data)\n",
    "        image = Image.open(BytesIO(image_bytes))\n",
    "        \n",
    "        # Convert to RGB if needed\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "            \n",
    "        return image\n",
    "    \n",
    "    def generate_caption(self, image, prompt):\n",
    "        \"\"\"Generate caption for the given image\"\"\"\n",
    "        # Process the image with the BLIP processor\n",
    "        inputs = self.processor(image, prompt, return_tensors=\"pt\").to(self.device, self.model_dtype)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Generate caption\n",
    "            generated_ids = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=50,\n",
    "                num_beams=5,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            caption = self.processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        return caption\n",
    "    \n",
    "    # Optional: Use OPT model to enhance generation\n",
    "    \"\"\"\n",
    "    def enhance_caption(self, caption):\n",
    "        \"\"\"Use OPT model to enhance the generated caption\"\"\"\n",
    "        if self.opt_model is None or self.opt_tokenizer is None:\n",
    "            return caption\n",
    "            \n",
    "        # Prepare input for OPT model\n",
    "        input_text = f\"Enhance this Mercedes car description: {caption}\\nEnhanced description:\"\n",
    "        input_ids = self.opt_tokenizer(input_text, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        \n",
    "        # Generate enhanced caption\n",
    "        with torch.no_grad():\n",
    "            outputs = self.opt_model.generate(\n",
    "                input_ids,\n",
    "                max_length=100,\n",
    "                num_beams=3,\n",
    "                temperature=0.7,\n",
    "                do_sample=True\n",
    "            )\n",
    "            enhanced_caption = self.opt_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract just the enhanced part\n",
    "        enhanced_part = enhanced_caption.split(\"Enhanced description:\")[-1].strip()\n",
    "        return enhanced_part if enhanced_part else caption\n",
    "    \"\"\"\n",
    "    \n",
    "    def predict(self, request: Dict) -> Dict:\n",
    "        \"\"\"Generate caption for the image in the request\"\"\"\n",
    "        if not self.ready:\n",
    "            return {\"error\": \"Model not loaded yet\"}\n",
    "        \n",
    "        instances = request.get(\"instances\", [])\n",
    "        responses = []\n",
    "        \n",
    "        for instance in instances:\n",
    "            try:\n",
    "                # Extract image data and prompt\n",
    "                image_data = instance.get(\"image\", None)\n",
    "                prompt = instance.get(\"prompt\", self.default_prompt)\n",
    "                \n",
    "                if image_data is None:\n",
    "                    responses.append({\"error\": \"No image data provided\"})\n",
    "                    continue\n",
    "                \n",
    "                # Process image\n",
    "                image = self.preprocess_image(image_data)\n",
    "                \n",
    "                # Generate caption\n",
    "                caption = self.generate_caption(image, prompt)\n",
    "                \n",
    "                # Optional: Enhance caption with OPT\n",
    "                # caption = self.enhance_caption(caption)\n",
    "                \n",
    "                # Add to responses\n",
    "                responses.append({\"caption\": caption})\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing request: {str(e)}\")\n",
    "                responses.append({\"error\": str(e)})\n",
    "        \n",
    "        return {\"predictions\": responses}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_server()\n",
    "\"\"\"\n",
    "\n",
    "# Save the predictor code to a file\n",
    "predictor_dir = os.path.join(model_dir, \"predictor\")\n",
    "os.makedirs(predictor_dir, exist_ok=True)\n",
    "with open(os.path.join(predictor_dir, \"model.py\"), \"w\") as f:\n",
    "    f.write(blip_predictor_code)\n",
    "\n",
    "print(f\"Created predictor code at {os.path.join(predictor_dir, 'model.py')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Dockerfile for Custom BLIP Predictor\n",
    "\n",
    "Now, we'll create a Dockerfile to package our custom predictor and model for KServe deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dockerfile for the custom predictor\n",
    "dockerfile_content = \"\"\"\n",
    "FROM pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    git \\\n",
    "    wget \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Install Python dependencies\n",
    "RUN pip install --no-cache-dir \\\n",
    "    kserve==0.10.1 \\\n",
    "    transformers==4.30.2 \\\n",
    "    pillow==9.5.0 \\\n",
    "    scikit-learn==1.2.2 \\\n",
    "    setuptools==67.8.0 \\\n",
    "    torchvision==0.15.2\n",
    "\n",
    "# Create model directory\n",
    "WORKDIR /app\n",
    "COPY predictor /app/predictor\n",
    "COPY model /mnt/models/model\n",
    "\n",
    "# Copy OPT model if it exists\n",
    "# COPY opt_model /mnt/models/opt_model\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONPATH=/app\n",
    "ENV STORAGE_URI=/mnt/models\n",
    "\n",
    "# Set entrypoint\n",
    "ENTRYPOINT [\"python\", \"/app/predictor/model.py\"]\n",
    "\"\"\"\n",
    "\n",
    "# Save the Dockerfile\n",
    "with open(os.path.join(model_dir, \"Dockerfile\"), \"w\") as f:\n",
    "    f.write(dockerfile_content)\n",
    "\n",
    "print(f\"Created Dockerfile at {os.path.join(model_dir, 'Dockerfile')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build and Push Docker Image\n",
    "\n",
    "Now, we'll build the Docker image and push it to a container registry. Note that this step requires Docker to be installed and configured correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define container registry and image details\n",
    "# Replace with your actual registry information\n",
    "REGISTRY = \"your-registry.azurecr.io\"  # Example: Docker Hub, Azure Container Registry, etc.\n",
    "IMAGE_NAME = f\"{REGISTRY}/{MODEL_NAME}:latest\"\n",
    "\n",
    "# Build Docker image\n",
    "!cd {model_dir} && docker build -t {IMAGE_NAME} .\n",
    "\n",
    "# Push Docker image to registry\n",
    "!docker push {IMAGE_NAME}\n",
    "\n",
    "print(f\"Built and pushed Docker image: {IMAGE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create KServe InferenceService\n",
    "\n",
    "Next, we'll create the KServe InferenceService definition to deploy our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create InferenceService yaml\n",
    "inference_service_yaml = f\"\"\"\n",
    "apiVersion: \"serving.kserve.io/v1beta1\"\n",
    "kind: \"InferenceService\"\n",
    "metadata:\n",
    "  name: \"{MODEL_NAME}\"\n",
    "  namespace: \"{NAMESPACE}\"\n",
    "  annotations:\n",
    "    serving.kserve.io/deploymentMode: \"ModelMesh\"\n",
    "spec:\n",
    "  predictor:\n",
    "    containers:\n",
    "      - name: kserve-container\n",
    "        image: \"{IMAGE_NAME}\"\n",
    "        resources:\n",
    "          limits:\n",
    "            cpu: \"4\"\n",
    "            memory: \"8Gi\"\n",
    "            nvidia.com/gpu: \"1\"\n",
    "          requests:\n",
    "            cpu: \"1\"\n",
    "            memory: \"4Gi\"\n",
    "            nvidia.com/gpu: \"1\"\n",
    "        args:\n",
    "          - --model_name={MODEL_NAME}\n",
    "          - --predictor_host=0.0.0.0:8080\n",
    "          - --http_port=8080\n",
    "        env:\n",
    "          - name: STORAGE_URI\n",
    "            value: /mnt/models\n",
    "        securityContext:\n",
    "          runAsUser: 1000\n",
    "\"\"\"\n",
    "\n",
    "# Save the InferenceService YAML\n",
    "inference_service_file = os.path.join(model_dir, f\"{MODEL_NAME}-inferenceservice.yaml\")\n",
    "with open(inference_service_file, \"w\") as f:\n",
    "    f.write(inference_service_yaml)\n",
    "\n",
    "print(f\"Created InferenceService YAML at {inference_service_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deploy the Model to KServe\n",
    "\n",
    "Now, we'll deploy the model to KServe using the Kubernetes API or kubectl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the InferenceService\n",
    "!kubectl apply -f {inference_service_file}\n",
    "\n",
    "# Wait for the service to be ready\n",
    "print(\"Waiting for InferenceService to be ready...\")\n",
    "!kubectl get inferenceservice {MODEL_NAME} -n {NAMESPACE}\n",
    "\n",
    "# Function to check if the service is ready\n",
    "def is_service_ready():\n",
    "    try:\n",
    "        result = !kubectl get inferenceservice {MODEL_NAME} -n {NAMESPACE} -o jsonpath='{{.status.conditions[?(@.type==\"Ready\")].status}}'\n",
    "        return result[0] == \"True\"\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Wait for the service to be ready (with timeout)\n",
    "timeout = 600  # 10 minutes\n",
    "start_time = time.time()\n",
    "while time.time() - start_time < timeout:\n",
    "    if is_service_ready():\n",
    "        print(f\"\\nInferenceService {MODEL_NAME} is ready!\")\n",
    "        break\n",
    "    print(\".\", end=\"\")\n",
    "    time.sleep(10)\n",
    "else:\n",
    "    print(f\"\\nTimeout reached. InferenceService {MODEL_NAME} may not be ready yet.\")\n",
    "    print(\"Check the status manually using kubectl:\")\n",
    "    print(f\"kubectl get inferenceservice {MODEL_NAME} -n {NAMESPACE}\")\n",
    "    print(f\"kubectl describe inferenceservice {MODEL_NAME} -n {NAMESPACE}\")\n",
    "\n",
    "# Get the service endpoint\n",
    "print(\"\\nGetting service URL...\")\n",
    "!kubectl get inferenceservice {MODEL_NAME} -n {NAMESPACE} -o jsonpath='{{.status.url}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test the Deployed Model\n",
    "\n",
    "Now that our model is deployed, let's test it with some sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the service URL\n",
    "service_url_result = !kubectl get inferenceservice {MODEL_NAME} -n {NAMESPACE} -o jsonpath='{{.status.url}}'\n",
    "service_url = service_url_result[0] if service_url_result else None\n",
    "\n",
    "if not service_url:\n",
    "    print(\"Could not get service URL. Please check the deployment status.\")\n",
    "else:\n",
    "    service_url = service_url + \"/v1/models/{MODEL_NAME}:predict\"\n",
    "    print(f\"Service endpoint: {service_url}\")\n",
    "\n",
    "# Function to encode image to base64\n",
    "def encode_image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        encoded_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    return encoded_image\n",
    "\n",
    "# Function to test the deployed model with a test image\n",
    "def test_with_image(image_path, prompt=\"This image shows\"):\n",
    "    try:\n",
    "        # Encode image to base64\n",
    "        encoded_image = encode_image_to_base64(image_path)\n",
    "        \n",
    "        # Create request JSON\n",
    "        request_data = {\n",
    "            \"instances\": [\n",
    "                {\n",
    "                    \"image\": encoded_image,\n",
    "                    \"prompt\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Send request to the service\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        import requests\n",
    "        response = requests.post(service_url, json=request_data, headers=headers)\n",
    "        \n",
    "        # Display the image\n",
    "        img = Image.open(image_path)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Display the caption\n",
    "        if response.status_code == 200:\n",
    "            response_json = response.json()\n",
    "            caption = response_json[\"predictions\"][0].get(\"caption\", \"No caption generated\")\n",
    "            plt.title(f\"Caption: {caption}\")\n",
    "            print(f\"Generated caption: {caption}\")\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "            plt.title(\"Error in caption generation\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Test with a sample image - users will need to provide their own test image path\n",
    "test_image_path = \"path/to/your/test/image.jpg\"  # Update this path to a local test image\n",
    "print(f\"Testing with image: {test_image_path}\")\n",
    "response = test_with_image(test_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Monitor Model Performance\n",
    "\n",
    "Finally, let's set up monitoring for our model using KServe's monitoring capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model service metrics\n",
    "print(\"Checking model service metrics...\")\n",
    "!kubectl get pods -n {NAMESPACE} -l serving.kserve.io/inferenceservice={MODEL_NAME}\n",
    "\n",
    "# Get the predictor pod name\n",
    "pod_result = !kubectl get pods -n {NAMESPACE} -l serving.kserve.io/inferenceservice={MODEL_NAME} -o jsonpath='{{.items[0].metadata.name}}'\n",
    "pod_name = pod_result[0] if pod_result else None\n",
    "\n",
    "if pod_name:\n",
    "    print(f\"\\nDescribe pod {pod_name}:\")\n",
    "    !kubectl describe pod {pod_name} -n {NAMESPACE}\n",
    "    \n",
    "    print(f\"\\nCheck logs for pod {pod_name}:\")\n",
    "    !kubectl logs {pod_name} -n {NAMESPACE} -c kserve-container --tail=50\n",
    "else:\n",
    "    print(\"Could not find predictor pod. Please check deployment status.\")\n",
    "\n",
    "print(\"\\nSet up monitoring dashboards in Grafana or Prometheus to track:\")\n",
    "print(\"- Request latency\")\n",
    "print(\"- Throughput\")\n",
    "print(\"- Error rate\")\n",
    "print(\"- Resource utilization (CPU, GPU, memory)\")\n",
    "print(\"\\nAnd create alerts for any anomalies or performance degradation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Clean Up Resources (Optional)\n",
    "\n",
    "If you need to clean up the deployed resources, run the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "def cleanup_resources():\n",
    "    print(\"Deleting InferenceService...\")\n",
    "    !kubectl delete inferenceservice {MODEL_NAME} -n {NAMESPACE}\n",
    "    \n",
    "    print(\"Cleaning up local files...\")\n",
    "    !rm -rf {model_dir}\n",
    "    \n",
    "    print(\"Resources cleaned up.\")\n",
    "\n",
    "# Uncomment to clean up resources\n",
    "# cleanup_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "In this notebook, we've accomplished the following:\n",
    "\n",
    "1. Connected to MLflow to retrieve our fine-tuned BLIP model\n",
    "2. Created a custom KServe predictor for our model\n",
    "3. Built and deployed a Docker container with our model\n",
    "4. Deployed the model to KServe on our Kubernetes cluster\n",
    "5. Tested the deployed model with sample images\n",
    "6. Set up monitoring for our model\n",
    "\n",
    "The deployed model is now available as a scalable API endpoint that can be integrated with other applications that need image captioning capabilities for Mercedes car images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 