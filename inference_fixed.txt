# Model Inference on First Image in Dataset

This file contains code to load the converted BLIP2 model from MLflow and perform inference on the first image in the dataset.

## 1. Set up imports and device

```python
import os
import mlflow
from mlflow.tracking import MlflowClient
import torch
import boto3
import json
import numpy as np
from PIL import Image
from io import BytesIO
import matplotlib.pyplot as plt
from torchvision import transforms

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
```

## 2. Connect to S3 to access the dataset

```python
# Configure S3 connection
object_storage_service_name = "source-images-service"
object_storage_namespace = ".ezdata-system"
resource_type = ".svc"
domain = ".cluster.local"
object_storage_port = "30000"

s3_endpoint_url = f"http://{object_storage_service_name}{object_storage_namespace}{resource_type}{domain}:{object_storage_port}"
print(f"S3 endpoint URL: {s3_endpoint_url}")

# Create S3 clients
s3_client = boto3.client('s3', endpoint_url=s3_endpoint_url)
s3_resource = boto3.resource('s3', endpoint_url=s3_endpoint_url)
```

## 3. Load the dataset information

```python
# Set bucket name
bucket_name = "poc-mercedes-gp"

# Load the dataset JSON file
file_key = "training/training_dataset.json"
response = s3_client.get_object(Bucket=bucket_name, Key=file_key)
content = response["Body"].read().decode("utf-8")
dataset = json.loads(content)

# Display information about the first image in the dataset
print("First image information:")
print(dataset[0])
```

## 4. Download and display the first image

```python
# Get the S3 key for the first image
first_image_key = dataset[0]['s3_key']
print(f"First image S3 key: {first_image_key}")

# Download the image
response = s3_client.get_object(Bucket=bucket_name, Key=first_image_key)
image_data = response['Body'].read()

# Convert to PIL Image
image = Image.open(BytesIO(image_data))

# Display the image
plt.figure(figsize=(10, 10))
plt.imshow(image)
plt.axis('off')
plt.title(f"Image: {first_image_key.split('/')[-1]}\nGround Truth: {dataset[0]['text']}")
plt.show()
```

## 5. Load the converted model from MLflow

```python
# Refresh token if needed
%update_token

# Get the latest version of the registered model
client = MlflowClient()
registered_model_name = "blip_ft_production"

# Get the latest version
latest_versions = client.get_latest_versions(registered_model_name)
latest_version = None
for version in latest_versions:
    if version.current_stage == "Production":
        latest_version = version
        break
    
if latest_version is None and len(latest_versions) > 0:
    latest_version = latest_versions[0]

if latest_version is None:
    raise ValueError(f"No versions found for model {registered_model_name}")

print(f"Using model version: {latest_version.version}, stage: {latest_version.current_stage}")

# Get the model's artifact URI
model_uri = f"models:/{registered_model_name}/{latest_version.version}"
print(f"Model URI: {model_uri}")

# Load the model as a PyTorch model
scripted_model = mlflow.pytorch.load_model(model_uri, map_location=device)
print("Model loaded successfully")
```

## 6. Preprocess the image manually (without LAVIS)

```python
# Define a custom preprocessing function based on standard BLIP2 preprocessing
def preprocess_image(image, size=224):
    # BLIP2 standard preprocessing
    transform = transforms.Compose([
        transforms.Resize((size, size)),
        transforms.ToTensor(),
        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), 
                             (0.26862954, 0.26130258, 0.27577711))
    ])
    
    # Convert to RGB if it's not already
    if image.mode != "RGB":
        image = image.convert("RGB")
        
    # Apply transformations
    tensor = transform(image).unsqueeze(0)  # Add batch dimension
    return tensor

# Preprocess the image
processed_image = preprocess_image(image).to(device)
print(f"Processed image shape: {processed_image.shape}")
```

## 7. Run inference with the model

```python
# Set model to evaluation mode
scripted_model.eval()

# Run inference
with torch.no_grad():
    features = scripted_model(processed_image)
    
print(f"Output features shape: {features.shape}")

# Displaying the first few features
print("First 10 feature values:")
print(features[0, :10].cpu().numpy())
```

## 8. Visualize Feature Map

```python
from sklearn.decomposition import PCA

# Convert features to numpy for PCA
features_np = features.cpu().numpy().squeeze()

# Apply PCA to reduce dimensionality for visualization
pca = PCA(n_components=3)
features_pca = pca.fit_transform(features_np)

# Create a 3D visualization
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

# Plot the first 100 points (or fewer if there are fewer)
num_points = min(100, features_pca.shape[0])
scatter = ax.scatter(features_pca[:num_points, 0], 
           features_pca[:num_points, 1], 
           features_pca[:num_points, 2],
           c=range(num_points), 
           cmap='viridis')

ax.set_title('PCA of Feature Vectors')
ax.set_xlabel('PC1')
ax.set_ylabel('PC2')
ax.set_zlabel('PC3')
plt.colorbar(scatter, label='Feature Index')
plt.show()

# Print variance explained by each component
print("Variance explained by principal components:")
print(pca.explained_variance_ratio_)
print(f"Total variance explained: {sum(pca.explained_variance_ratio_):.2f}")
```

## 9. Try to use the features for inference (optional)

```python
# If the model exposes a text_decoder or projection layer, we might try to use it
# This is a simplified example and might need adjustment based on the actual model structure
try:
    if hasattr(scripted_model, "generate_text") and callable(scripted_model.generate_text):
        # Some models might have a text generation function
        text = scripted_model.generate_text(features)
        print(f"Generated text: {text}")
        print(f"Ground truth: {dataset[0]['text']}")
    else:
        print("The model doesn't have a direct text generation method.")
        print("The features might need to be passed to a separate text generation model.")
except Exception as e:
    print(f"Could not generate text: {e}")
```

## 10. Alternative: Try direct image input if feature extraction doesn't work

```python
# If the above doesn't work, the model might expect direct image input for full processing
try:
    # Some TorchScript models might have different input expectations
    with torch.no_grad():
        alternative_output = scripted_model(processed_image)
    
    print("Alternative direct input output:")
    if isinstance(alternative_output, tuple):
        print(f"Output is a tuple of length {len(alternative_output)}")
        for i, item in enumerate(alternative_output):
            if torch.is_tensor(item):
                print(f"Item {i} shape: {item.shape}")
            else:
                print(f"Item {i} type: {type(item)}")
    elif torch.is_tensor(alternative_output):
        print(f"Output shape: {alternative_output.shape}")
    else:
        print(f"Output type: {type(alternative_output)}")
except Exception as e:
    print(f"Alternative approach failed: {e}")
```

## 11. Summary

We have:
1. Loaded the first image from the S3 dataset
2. Displayed the image with its ground truth caption
3. Loaded the converted model from MLflow
4. Performed manual preprocessing without using LAVIS
5. Run inference with the model
6. Visualized the extracted features

The features extracted by the BLIP2 model can be used for tasks like image classification, captioning, or retrieval, but might require additional processing or models for generating the final text output.
