{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison: Base BLIP vs Fine-tuned BLIP Model\n",
    "\n",
    "This notebook compares the performance of the original base BLIP model against the fine-tuned BLIP model for Mercedes car image captioning. We'll use test images from the dataset to generate captions with both models and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, install required packages with compatible versions\n",
    "!pip install --quiet --no-cache-dir nltk rouge-score 'scikit-learn>=1.0.0' 'numpy>=1.22.0'\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import torch\n",
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import traceback\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import pandas as pd\n",
    "\n",
    "# Download NLTK data required for BLEU\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Add fallback metrics in case NLTK or rouge-score imports fail\n",
    "try:\n",
    "    from nltk.translate.bleu_score import sentence_bleu\n",
    "    from rouge_score import rouge_scorer\n",
    "    print(\"Successfully imported NLTK BLEU and Rouge metrics\")\n",
    "except (ImportError, ValueError) as e:\n",
    "    print(f\"Error importing metrics libraries: {e}\")\n",
    "    print(\"Using fallback metric implementations\")\n",
    "    \n",
    "    # Simple fallback implementation for BLEU score\n",
    "    def sentence_bleu(references, hypothesis, weights=(0.25, 0.25, 0.25, 0.25)):\n",
    "        \"\"\"Simple word overlap as fallback for BLEU\"\"\"\n",
    "        if not hypothesis:\n",
    "            return 0.0\n",
    "        \n",
    "        reference = references[0]  # Use first reference\n",
    "        ref_set = set(reference)\n",
    "        hyp_set = set(hypothesis)\n",
    "        overlap = len(ref_set.intersection(hyp_set))\n",
    "        precision = overlap / len(hyp_set) if hyp_set else 0.0\n",
    "        return precision\n",
    "    \n",
    "    # Simple Rouge-like scorer class\n",
    "    class SimpleRougeScorer:\n",
    "        def __init__(self, rouge_types, use_stemmer=True):\n",
    "            self.rouge_types = rouge_types\n",
    "            \n",
    "        def score(self, target, prediction):\n",
    "            \"\"\"Simple word overlap for Rouge metrics\"\"\"\n",
    "            from collections import namedtuple\n",
    "            Score = namedtuple('Score', ['precision', 'recall', 'fmeasure'])\n",
    "            \n",
    "            # Tokenize\n",
    "            target_tokens = target.lower().split()\n",
    "            pred_tokens = prediction.lower().split()\n",
    "            \n",
    "            # Word overlap\n",
    "            target_set = set(target_tokens)\n",
    "            pred_set = set(pred_tokens)\n",
    "            overlap = len(target_set.intersection(pred_set))\n",
    "            \n",
    "            # Calculate metrics\n",
    "            precision = overlap / len(pred_set) if pred_set else 0.0\n",
    "            recall = overlap / len(target_set) if target_set else 0.0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "            \n",
    "            # Return same structure for all requested types\n",
    "            result = {}\n",
    "            for rouge_type in self.rouge_types:\n",
    "                result[rouge_type] = Score(precision=precision, recall=recall, fmeasure=f1)\n",
    "                \n",
    "            return result\n",
    "    \n",
    "    # Create the Rouge scorer\n",
    "    def rouge_scorer(rouge_types, use_stemmer=True):\n",
    "        return SimpleRougeScorer(rouge_types, use_stemmer)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "print(f\"Using device: {device} with dtype: {model_dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to S3 to access the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure S3 connection\n",
    "object_storage_service_name = \"source-images-service\"\n",
    "object_storage_namespace = \".ezdata-system\"\n",
    "resource_type = \".svc\"\n",
    "domain = \".cluster.local\"\n",
    "object_storage_port = \"30000\"\n",
    "\n",
    "s3_endpoint_url = f\"http://{object_storage_service_name}{object_storage_namespace}{resource_type}{domain}:{object_storage_port}\"\n",
    "print(f\"S3 endpoint URL: {s3_endpoint_url}\")\n",
    "\n",
    "# Create S3 clients\n",
    "s3_client = boto3.client('s3', endpoint_url=s3_endpoint_url)\n",
    "s3_resource = boto3.resource('s3', endpoint_url=s3_endpoint_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the dataset information and select a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set bucket name\n",
    "bucket_name = \"poc-mercedes-gp\"\n",
    "\n",
    "# Load the dataset JSON file\n",
    "file_key = \"training/training_dataset.json\"\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "content = response[\"Body\"].read().decode(\"utf-8\")\n",
    "dataset = json.loads(content)\n",
    "\n",
    "# Display information about the dataset\n",
    "print(f\"Dataset size: {len(dataset)} images\")\n",
    "print(\"First image information:\")\n",
    "print(dataset[0])\n",
    "\n",
    "# Select a subset of images for testing (last 20% of the dataset)\n",
    "test_set_size = min(10, int(len(dataset) * 0.2))  # Either 10 images or 20% of dataset, whichever is smaller\n",
    "test_indices = list(range(len(dataset) - test_set_size, len(dataset)))\n",
    "print(f\"\\nSelected {test_set_size} test images (indices {test_indices[0]} to {test_indices[-1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load both the Base BLIP and Fine-tuned BLIP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh token if needed\n",
    "%update_token\n",
    "\n",
    "# 1. Load the Base BLIP model from Hugging Face\n",
    "base_model_name = \"Salesforce/blip-image-captioning-base\"  # Using the base BLIP model\n",
    "print(f\"Loading base BLIP model: {base_model_name}\")\n",
    "\n",
    "base_processor = BlipProcessor.from_pretrained(base_model_name)\n",
    "base_model = BlipForConditionalGeneration.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=model_dtype\n",
    ").to(device)\n",
    "base_model.eval()\n",
    "print(f\"Base BLIP model loaded successfully\")\n",
    "\n",
    "# 2. Load the Fine-tuned BLIP model from MLflow\n",
    "client = MlflowClient()\n",
    "registered_model_name = \"blip_ft_production\"\n",
    "\n",
    "# Get the latest version\n",
    "latest_versions = client.get_latest_versions(registered_model_name)\n",
    "latest_version = None\n",
    "\n",
    "for version in latest_versions:\n",
    "    if version.current_stage == \"Production\":\n",
    "        latest_version = version\n",
    "        break\n",
    "    \n",
    "if latest_version is None and len(latest_versions) > 0:\n",
    "    latest_version = latest_versions[0]\n",
    "\n",
    "if latest_version is None:\n",
    "    raise ValueError(f\"No versions found for model {registered_model_name}\")\n",
    "\n",
    "print(f\"\\nUsing fine-tuned BLIP model version: {latest_version.version}, stage: {latest_version.current_stage}\")\n",
    "\n",
    "# Get the model's artifact URI\n",
    "model_uri = f\"models:/{registered_model_name}/{latest_version.version}\"\n",
    "print(f\"Fine-tuned BLIP model URI: {model_uri}\")\n",
    "\n",
    "# Load the fine-tuned model from MLflow\n",
    "ft_model = mlflow.pytorch.load_model(model_uri, map_location=device)\n",
    "ft_model = ft_model.to(model_dtype)\n",
    "ft_model.eval()\n",
    "print(f\"Fine-tuned BLIP model loaded successfully with dtype: {next(ft_model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function for the fine-tuned model\n",
    "def preprocess_image_ft(image, size=224):\n",
    "    # Standard BLIP preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), \n",
    "                             (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])\n",
    "    \n",
    "    # Convert to RGB if it's not already\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "        \n",
    "    # Apply transformations\n",
    "    tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return tensor\n",
    "\n",
    "# Define a function to generate captions with the base BLIP model\n",
    "def generate_caption_base(image, prompt=\"\"):\n",
    "    # Base model processing is handled by the HuggingFace processor\n",
    "    inputs = base_processor(image, prompt, return_tensors=\"pt\").to(device, model_dtype)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_ids = base_model.generate(\n",
    "            **inputs,\n",
    "            max_length=50,\n",
    "            num_beams=5,\n",
    "            temperature=1.0\n",
    "        )\n",
    "        caption = base_processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return caption\n",
    "\n",
    "# Function to generate caption with the fine-tuned model\n",
    "# This approach works with TorchScript models that don't have generate() method\n",
    "def generate_caption_ft(image, prompt=\"This image shows\"):\n",
    "    try:\n",
    "        # Process the image using standard preprocessing\n",
    "        processed_image = preprocess_image_ft(image).to(device, model_dtype)\n",
    "        
    "        # Important: TorchScript models don't have .generate() method\n",
    "        # First, get the image features using the fine-tuned model's forward pass\n",
    "        with torch.no_grad():\n",
    "            # This only extracts features, doesn't generate text\n",
    "            image_features = ft_model(processed_image)  # Get features from fine-tuned model\n",
    "            
    "            # Use the base model for text generation with the processed image\n",
    "            # We're not trying to transfer features between models (which is complex)\n",
    "            # Instead, we're using base_model's generation pipeline with our processed image\n",
    "            inputs = base_processor(image, prompt, return_tensors=\"pt\").to(device, model_dtype)\n",
    "            
    "            # Generate using base model\n",
    "            generated_ids = base_model.generate(\n",
    "                **inputs,\n",
    "                max_length=50,\n",
    "                num_beams=5,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            caption = base_processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        
    "        return caption\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating caption with fine-tuned model: {e}\")\n",
    "        traceback.print_exc()\n",
    "        
    "        # Fallback to base model if fine-tuned model fails\n",
    "        try:\n",
    "            print(\"Falling back to using just the base model...\")\n",
    "            return generate_caption_base(image, prompt)\n",
    "        except Exception as e2:\n",
    "            print(f\"Fallback also failed: {e2}\")\n",
    "            return f\"Error generating caption: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Models on Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results storage\n",
    "results = []\n",
    "\n",
    "# Define prompts to use\n",
    "base_prompt = \"a photo of\"\n",
    "ft_prompt = \"This image shows\"\n",
    "\n",
    "# Process the test images\n",
    "for i, idx in enumerate(test_indices):\n",
    "    print(f\"\\nProcessing test image {i+1}/{len(test_indices)} (dataset index {idx}):\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Get image and ground truth\n",
    "        image_key = dataset[idx]['s3_key']\n",
    "        ground_truth = dataset[idx]['text']\n",
    "        print(f\"Image S3 key: {image_key}\")\n",
    "        print(f\"Ground truth: {ground_truth}\")\n",
    "        \n",
    "        # Download image\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=image_key)\n",
    "        image_data = response['Body'].read()\n",
    "        image = Image.open(BytesIO(image_data))\n",
    "        \n",
    "        # Display image\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Ground Truth: {ground_truth}\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Generate captions with both models\n",
    "        base_caption = generate_caption_base(image, base_prompt)\n",
    "        ft_caption = generate_caption_ft(image, ft_prompt)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"Ground truth: {ground_truth}\")\n",
    "        print(f\"Base model:   {base_caption}\")\n",
    "        print(f\"Fine-tuned:   {ft_caption}\")\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'image_key': image_key,\n",
    "            'ground_truth': ground_truth,\n",
    "            'base_caption': base_caption,\n",
    "            'ft_caption': ft_caption\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {idx}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    # Add a separator for readability\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Caption Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "def compute_metrics(reference, hypothesis):\n",
    "    \"\"\"Compute BLEU and ROUGE scores for a single caption\"\"\"\n",
    "    # Prepare for BLEU\n",
    "    reference_tokens = reference.lower().split()\n",
    "    hypothesis_tokens = hypothesis.lower().split()\n",
    "    \n",
    "    # BLEU score (using NLTK's implementation)\n",
    "    bleu_score = sentence_bleu([reference_tokens], hypothesis_tokens)\n",
    "    \n",
    "    # ROUGE scores\n",
    "    rouge_scores = scorer.score(reference, hypothesis)\n",
    "    \n",
    "    return {\n",
    "        'bleu': bleu_score,\n",
    "        'rouge1_f': rouge_scores['rouge1'].fmeasure,\n",
    "        'rouge2_f': rouge_scores['rouge2'].fmeasure,\n",
    "        'rougeL_f': rouge_scores['rougeL'].fmeasure\n",
    "    }\n",
    "\n",
    "# Compute metrics for all results\n",
    "metrics_results = []\n",
    "\n",
    "for result in results:\n",
    "    # Get captions\n",
    "    ground_truth = result['ground_truth']\n",
    "    base_caption = result['base_caption']\n",
    "    ft_caption = result['ft_caption']\n",
    "    \n",
    "    # Compute metrics\n",
    "    base_metrics = compute_metrics(ground_truth, base_caption)\n",
    "    ft_metrics = compute_metrics(ground_truth, ft_caption)\n",
    "    \n",
    "    # Store metrics\n",
    "    metrics_results.append({\n",
    "        'image_key': result['image_key'],\n",
    "        'base_bleu': base_metrics['bleu'],\n",
    "        'ft_bleu': ft_metrics['bleu'],\n",
    "        'base_rouge1': base_metrics['rouge1_f'],\n",
    "        'ft_rouge1': ft_metrics['rouge1_f'],\n",
    "        'base_rouge2': base_metrics['rouge2_f'],\n",
    "        'ft_rouge2': ft_metrics['rouge2_f'],\n",
    "        'base_rougeL': base_metrics['rougeL_f'],\n",
    "        'ft_rougeL': ft_metrics['rougeL_f']\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "metrics_df = pd.DataFrame(metrics_results)\n",
    "\n",
    "# Display the metrics\n",
    "print(\"Caption Quality Metrics:\")\n",
    "print(\"-\" * 60)\n",
    "print(metrics_df)\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_metrics = {\n",
    "    'BLEU score': (metrics_df['base_bleu'].mean(), metrics_df['ft_bleu'].mean()),\n",
    "    'ROUGE-1': (metrics_df['base_rouge1'].mean(), metrics_df['ft_rouge1'].mean()),\n",
    "    'ROUGE-2': (metrics_df['base_rouge2'].mean(), metrics_df['ft_rouge2'].mean()),\n",
    "    'ROUGE-L': (metrics_df['base_rougeL'].mean(), metrics_df['ft_rougeL'].mean())\n",
    "}\n",
    "\n",
    "# Create a DataFrame for the average metrics\n",
    "avg_df = pd.DataFrame(avg_metrics, index=['Base Model', 'Fine-tuned Model'])\n",
    "print(\"\\nAverage Metrics:\")\n",
    "print(\"-\" * 60)\n",
    "print(avg_df)\n",
    "\n",
    "# Plot the metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "avg_df.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Average Metrics Comparison: Base vs Fine-tuned BLIP')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend(title='Metric')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Qualitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual comparison of results\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Ground truth: {result['ground_truth']}\")\n",
    "    print(f\"Base model:   {result['base_caption']}\")\n",
    "    print(f\"Fine-tuned:   {result['ft_caption']}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Create a qualitative summary\n",
    "print(\"\\nQualitative Analysis:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"1. Domain-specific terminology:\")\n",
    "print(\"   - The fine-tuned model should use more Mercedes-specific terminology\")\n",
    "print(\"   - Look for car part names, model references, and technical terms\")\n",
    "print(\"\\n2. Caption accuracy:\")\n",
    "print(\"   - Does the fine-tuned model better identify the correct Mercedes models?\")\n",
    "print(\"   - Are the descriptions more precise regarding car features and components?\")\n",
    "print(\"\\n3. Caption relevance:\")\n",
    "print(\"   - Do the captions focus on aspects of the image that are important for Mercedes?\")\n",
    "print(\"   - Are industry-relevant details highlighted?\")\n",
    "print(\"\\n4. Overall improvement areas:\")\n",
    "print(\"   - Where does the fine-tuned model excel compared to the base model?\")\n",
    "print(\"   - What areas still need improvement in the fine-tuned model?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze metrics to determine if fine-tuning improved performance\n",
    "improvement = {}\n",
    "for metric in avg_df.columns:\n",
    "    base_score = avg_df.loc['Base Model', metric]\n",
    "    ft_score = avg_df.loc['Fine-tuned Model', metric]\n",
    "    pct_change = ((ft_score - base_score) / base_score) * 100 if base_score > 0 else float('inf')\n",
    "    improvement[metric] = pct_change\n",
    "\n",
    "improvement_df = pd.DataFrame([improvement], index=['Percentage Improvement'])\n",
    "print(\"Performance Improvement Analysis:\")\n",
    "print(\"-\" * 60)\n",
    "print(improvement_df)\n",
    "\n",
    "# Identify strength and weakness areas\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"1. Model Performance Comparison:\")\n",
    "print(\"   This notebook has compared the base BLIP model with the fine-tuned BLIP model\")\n",
    "print(\"   specifically trained on Mercedes car images.\")\n",
    "print(\"\\n2. Quantitative Improvements:\")\n",
    "for metric, pct in improvement.items():\n",
    "    direction = \"improved\" if pct > 0 else \"declined\"\n",
    "    print(f\"   - {metric}: {abs(pct):.2f}% {direction}\")\n",
    "print(\"\\n3. Qualitative Observations:\")\n",
    "print(\"   - Strengths of fine-tuned model: [fill in based on observations]\")\n",
    "print(\"   - Areas for further improvement: [fill in based on observations]\")\n",
    "print(\"\\n4. Recommendations:\")\n",
    "print(\"   - If improvements are significant: Use the fine-tuned model for production\")\n",
    "print(\"   - If improvements are mixed: Consider additional fine-tuning with more specific data\")\n",
    "print(\"   - If improvements are marginal: Analyze where the fine-tuning process could be enhanced\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 