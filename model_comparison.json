{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison: Base BLIP vs Fine-tuned BLIP Model\n",
    "\n",
    "This notebook compares the performance of the original base BLIP model against the fine-tuned BLIP model for Mercedes car image captioning. We'll use test images from the dataset to generate captions with both models and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import torch\n",
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import traceback\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "print(f\"Using device: {device} with dtype: {model_dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to S3 to access the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure S3 connection\n",
    "object_storage_service_name = \"source-images-service\"\n",
    "object_storage_namespace = \".ezdata-system\"\n",
    "resource_type = \".svc\"\n",
    "domain = \".cluster.local\"\n",
    "object_storage_port = \"30000\"\n",
    "\n",
    "s3_endpoint_url = f\"http://{object_storage_service_name}{object_storage_namespace}{resource_type}{domain}:{object_storage_port}\"\n",
    "print(f\"S3 endpoint URL: {s3_endpoint_url}\")\n",
    "\n",
    "# Create S3 clients\n",
    "s3_client = boto3.client('s3', endpoint_url=s3_endpoint_url)\n",
    "s3_resource = boto3.resource('s3', endpoint_url=s3_endpoint_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the dataset information and select a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set bucket name\n",
    "bucket_name = \"poc-mercedes-gp\"\n",
    "\n",
    "# Load the dataset JSON file\n",
    "file_key = \"training/training_dataset.json\"\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "content = response[\"Body\"].read().decode(\"utf-8\")\n",
    "dataset = json.loads(content)\n",
    "\n",
    "# Display information about the dataset\n",
    "print(f\"Dataset size: {len(dataset)} images\")\n",
    "print(\"First image information:\")\n",
    "print(dataset[0])\n",
    "\n",
    "# Select a subset of images for testing (last 20% of the dataset)\n",
    "test_set_size = min(10, int(len(dataset) * 0.2))  # Either 10 images or 20% of dataset, whichever is smaller\n",
    "test_indices = list(range(len(dataset) - test_set_size, len(dataset)))\n",
    "print(f\"\\nSelected {test_set_size} test images (indices {test_indices[0]} to {test_indices[-1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load both the Base BLIP and Fine-tuned BLIP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh token if needed\n",
    "%update_token\n",
    "\n",
    "# 1. Load the Base BLIP model from Hugging Face\n",
    "base_model_name = \"Salesforce/blip-image-captioning-base\"  # Using the base BLIP model\n",
    "print(f\"Loading base BLIP model: {base_model_name}\")\n",
    "\n",
    "base_processor = BlipProcessor.from_pretrained(base_model_name)\n",
    "base_model = BlipForConditionalGeneration.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=model_dtype\n",
    ").to(device)\n",
    "base_model.eval()\n",
    "print(f\"Base BLIP model loaded successfully\")\n",
    "\n",
    "# 2. Load the Fine-tuned BLIP model from MLflow\n",
    "client = MlflowClient()\n",
    "registered_model_name = \"blip_ft_production\"\n",
    "\n",
    "# Get the latest version\n",
    "latest_versions = client.get_latest_versions(registered_model_name)\n",
    "latest_version = None\n",
    "\n",
    "for version in latest_versions:\n",
    "    if version.current_stage == \"Production\":\n",
    "        latest_version = version\n",
    "        break\n",
    "    \n",
    "if latest_version is None and len(latest_versions) > 0:\n",
    "    latest_version = latest_versions[0]\n",
    "\n",
    "if latest_version is None:\n",
    "    raise ValueError(f\"No versions found for model {registered_model_name}\")\n",
    "\n",
    "print(f\"\\nUsing fine-tuned BLIP model version: {latest_version.version}, stage: {latest_version.current_stage}\")\n",
    "\n",
    "# Get the model's artifact URI\n",
    "model_uri = f\"models:/{registered_model_name}/{latest_version.version}\"\n",
    "print(f\"Fine-tuned BLIP model URI: {model_uri}\")\n",
    "\n",
    "# Load the fine-tuned model from MLflow\n",
    "ft_model = mlflow.pytorch.load_model(model_uri, map_location=device)\n",
    "ft_model = ft_model.to(model_dtype)\n",
    "ft_model.eval()\n",
    "print(f\"Fine-tuned BLIP model loaded successfully with dtype: {next(ft_model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function for the fine-tuned model\n",
    "def preprocess_image_ft(image, size=224):\n",
    "    # Standard BLIP2 preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), \n",
    "                             (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])\n",
    "    \n",
    "    # Convert to RGB if it's not already\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "        \n",
    "    # Apply transformations\n",
    "    tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return tensor\n",
    "\n",
    "# Define a function to generate captions with the base BLIP model\n",
    "def generate_caption_base(image, prompt=\"\"):\n",
    "    # Base model processing is handled by the HuggingFace processor\n",
    "    inputs = base_processor(image, prompt, return_tensors=\"pt\").to(device, model_dtype)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_ids = base_model.generate(\n",
    "            **inputs,\n",
    "            max_length=50,\n",
    "            num_beams=5,\n",
    "            temperature=1.0\n",
    "        )\n",
    "        caption = base_processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return caption\n",
    "\n",
    "# For fine-tuned model, we need to use a similar approach as in the inference notebook\n",
    "# Let's use the OPT language model for generation\n",
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "\n",
    "# Load the OPT language model\n",
    "model_id = \"facebook/opt-1.3b\"\n",
    "print(f\"Loading language model: {model_id}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "language_model = OPTForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=model_dtype,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "language_model.eval()\n",
    "\n",
    "print(f\"Language model loaded with {sum(p.numel() for p in language_model.parameters())/1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for sampling\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering \"\"\"\n",
    "    # Handle both single and batch dimensions\n",
    "    if logits.dim() > 1:\n",
    "        # Apply filtering to each item in the batch\n",
    "        filtered_logits = logits.clone()\n",
    "        for batch_idx in range(logits.size(0)):\n",
    "            filtered_logits[batch_idx] = top_k_top_p_filtering(\n",
    "                logits[batch_idx], top_k=top_k, top_p=top_p, filter_value=filter_value\n",
    "            )\n",
    "        return filtered_logits\n",
    "    \n",
    "    assert logits.dim() == 1  # Handle scalar inputs\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    \n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    \n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        \n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        \n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    \n",
    "    return logits\n",
    "\n",
    "def generate_with_cross_attention(image_features, prompt, max_length=30, existing_projection_layer=None, temperature=0.7):\n",
    "    # Tokenize prompt\n",
    "    encoded_prompt = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = encoded_prompt.input_ids\n",
    "    \n",
    "    # Get model's hidden dimension\n",
    "    hidden_size = language_model.config.hidden_size\n",
    "    \n",
    "    # Use the provided projection layer or create a new one\n",
    "    if existing_projection_layer is not None and isinstance(existing_projection_layer, torch.nn.Module):\n",
    "        if existing_projection_layer.in_features == image_features.shape[-1] and existing_projection_layer.out_features == hidden_size:\n",
    "            projection_layer = existing_projection_layer\n",
    "        else:\n",
    "            projection_layer = torch.nn.Linear(image_features.shape[-1], hidden_size).to(device).to(model_dtype)\n",
    "            torch.nn.init.normal_(projection_layer.weight, std=0.01)\n",
    "            torch.nn.init.zeros_(projection_layer.bias)\n",
    "    else:\n",
    "        projection_layer = torch.nn.Linear(image_features.shape[-1], hidden_size).to(device).to(model_dtype)\n",
    "        torch.nn.init.normal_(projection_layer.weight, std=0.01)\n",
    "        torch.nn.init.zeros_(projection_layer.bias)\n",
    "    \n",
    "    # Ensure image features have the correct dtype\n",
    "    if image_features.dtype != projection_layer.weight.dtype:\n",
    "        image_features = image_features.to(projection_layer.weight.dtype)\n",
    "        \n",
    "    projected_img_features = projection_layer(image_features)\n",
    "    \n",
    "    # Initialize generation\n",
    "    cur_len = input_ids.shape[1]\n",
    "    context = input_ids.clone()\n",
    "    \n",
    "    # Generate one token at a time using a simplified approach\n",
    "    while cur_len < max_length:\n",
    "        with torch.no_grad():\n",
    "            # Use a simpler approach - get the embeddings directly\n",
    "            outputs = language_model(input_ids=context)\n",
    "            \n",
    "            # Get logits from the output\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            # Modify logits based on image features\n",
    "            # Convert to same dtype if needed\n",
    "            if projected_img_features.dtype != next_token_logits.dtype:\n",
    "                projected_img_features = projected_img_features.to(next_token_logits.dtype)\n",
    "            \n",
    "            # Scale and add the image features influence through a simple linear projection to the logits space\n",
    "            image_logits = torch.matmul(projected_img_features, language_model.lm_head.weight.t())\n",
    "            \n",
    "            # Combine model logits with image influence\n",
    "            combined_logits = next_token_logits + 0.1 * image_logits\n",
    "            \n",
    "            # Sample next token (with temperature)\n",
    "            combined_logits = combined_logits / temperature\n",
    "            filtered_logits = top_k_top_p_filtering(combined_logits, top_k=50, top_p=0.9)\n",
    "            probs = torch.nn.functional.softmax(filtered_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            context = torch.cat([context, next_token], dim=1)\n",
    "            cur_len += 1\n",
    "            \n",
    "            # Stop if we generate an EOS token\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(context[0], skip_special_tokens=True)\n",
    "    return generated_text, projection_layer\n",
    "\n",
    "# Function to generate caption with the fine-tuned model\n",
    "def generate_caption_ft(image, prompt=\"This image shows\", projection_layer=None):\n",
    "    # Process image\n",
    "    processed_image = preprocess_image_ft(image).to(device).to(model_dtype)\n",
    "    \n",
    "    # Extract features with fine-tuned BLIP model\n",
    "    with torch.no_grad():\n",
    "        image_features = ft_model(processed_image)\n",
    "        # Use the class token (first token) for generation\n",
    "        class_token = image_features[:, 0, :].to(device)\n",
    "    \n",
    "    # Generate caption using cross-attention\n",
    "    try:\n",
    "        caption, updated_projection_layer = generate_with_cross_attention(\n",
    "            class_token,\n",
    "            prompt,\n",
    "            existing_projection_layer=projection_layer,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return caption, updated_projection_layer\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating caption with fine-tuned model: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return \"Error generating caption\", projection_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Models on Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize projection layer for fine-tuned model\n",
    "projection_layer = None\n",
    "\n",
    "# Initialize results storage\n",
    "results = []\n",
    "\n",
    "# Define prompts to use\n",
    "base_prompt = \"a photo of\"\n",
    "ft_prompt = \"This image shows\"\n",
    "\n",
    "# Process the test images\n",
    "for i, idx in enumerate(test_indices):\n",
    "    print(f\"\\nProcessing test image {i+1}/{len(test_indices)} (dataset index {idx}):\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Get image and ground truth\n",
    "        image_key = dataset[idx]['s3_key']\n",
    "        ground_truth = dataset[idx]['text']\n",
    "        print(f\"Image S3 key: {image_key}\")\n",
    "        print(f\"Ground truth: {ground_truth}\")\n",
    "        \n",
    "        # Download image\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=image_key)\n",
    "        image_data = response['Body'].read()\n",
    "        image = Image.open(BytesIO(image_data))\n",
    "        \n",
    "        # Display image\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Ground Truth: {ground_truth}\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Generate captions with both models\n",
    "        base_caption = generate_caption_base(image, base_prompt)\n",
    "        ft_caption, projection_layer = generate_caption_ft(image, ft_prompt, projection_layer)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"Ground truth: {ground_truth}\")\n",
    "        print(f\"Base model:   {base_caption}\")\n",
    "        print(f\"Fine-tuned:   {ft_caption}\")\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'image_key': image_key,\n",
    "            'ground_truth': ground_truth,\n",
    "            'base_caption': base_caption,\n",
    "            'ft_caption': ft_caption\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {idx}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    # Add a separator for readability\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Caption Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "def compute_metrics(reference, hypothesis):\n",
    "    \"\"\"Compute BLEU and ROUGE scores for a single caption\"\"\"\n",
    "    # Prepare for BLEU\n",
    "    reference_tokens = reference.lower().split()\n",
    "    hypothesis_tokens = hypothesis.lower().split()\n",
    "    \n",
    "    # BLEU score (using NLTK's implementation)\n",
    "    bleu_score = sentence_bleu([reference_tokens], hypothesis_tokens)\n",
    "    \n",
    "    # ROUGE scores\n",
    "    rouge_scores = scorer.score(reference, hypothesis)\n",
    "    \n",
    "    return {\n",
    "        'bleu': bleu_score,\n",
    "        'rouge1_f': rouge_scores['rouge1'].fmeasure,\n",
    "        'rouge2_f': rouge_scores['rouge2'].fmeasure,\n",
    "        'rougeL_f': rouge_scores['rougeL'].fmeasure\n",
    "    }\n",
    "\n",
    "# Compute metrics for all results\n",
    "metrics_results = []\n",
    "\n",
    "for result in results:\n",
    "    # Get captions\n",
    "    ground_truth = result['ground_truth']\n",
    "    base_caption = result['base_caption']\n",
    "    ft_caption = result['ft_caption']\n",
    "    \n",
    "    # Compute metrics\n",
    "    base_metrics = compute_metrics(ground_truth, base_caption)\n",
    "    ft_metrics = compute_metrics(ground_truth, ft_caption)\n",
    "    \n",
    "    # Store metrics\n",
    "    metrics_results.append({\n",
    "        'image_key': result['image_key'],\n",
    "        'base_bleu': base_metrics['bleu'],\n",
    "        'ft_bleu': ft_metrics['bleu'],\n",
    "        'base_rouge1': base_metrics['rouge1_f'],\n",
    "        'ft_rouge1': ft_metrics['rouge1_f'],\n",
    "        'base_rouge2': base_metrics['rouge2_f'],\n",
    "        'ft_rouge2': ft_metrics['rouge2_f'],\n",
    "        'base_rougeL': base_metrics['rougeL_f'],\n",
    "        'ft_rougeL': ft_metrics['rougeL_f']\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "metrics_df = pd.DataFrame(metrics_results)\n",
    "\n",
    "# Display the metrics\n",
    "print(\"Caption Quality Metrics:\")\n",
    "print(\"-\" * 60)\n",
    "print(metrics_df)\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_metrics = {\n",
    "    'BLEU score': (metrics_df['base_bleu'].mean(), metrics_df['ft_bleu'].mean()),\n",
    "    'ROUGE-1': (metrics_df['base_rouge1'].mean(), metrics_df['ft_rouge1'].mean()),\n",
    "    'ROUGE-2': (metrics_df['base_rouge2'].mean(), metrics_df['ft_rouge2'].mean()),\n",
    "    'ROUGE-L': (metrics_df['base_rougeL'].mean(), metrics_df['ft_rougeL'].mean())\n",
    "}\n",
    "\n",
    "# Create a DataFrame for the average metrics\n",
    "avg_df = pd.DataFrame(avg_metrics, index=['Base Model', 'Fine-tuned Model'])\n",
    "print(\"\\nAverage Metrics:\")\n",
    "print(\"-\" * 60)\n",
    "print(avg_df)\n",
    "\n",
    "# Plot the metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "avg_df.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Average Metrics Comparison: Base vs Fine-tuned BLIP')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend(title='Metric')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Qualitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual comparison of results\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Ground truth: {result['ground_truth']}\")\n",
    "    print(f\"Base model:   {result['base_caption']}\")\n",
    "    print(f\"Fine-tuned:   {result['ft_caption']}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Create a qualitative summary\n",
    "print(\"\\nQualitative Analysis:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"1. Domain-specific terminology:\")\n",
    "print(\"   - The fine-tuned model should use more Mercedes-specific terminology\")\n",
    "print(\"   - Look for car part names, model references, and technical terms\")\n",
    "print(\"\\n2. Caption accuracy:\")\n",
    "print(\"   - Does the fine-tuned model better identify the correct Mercedes models?\")\n",
    "print(\"   - Are the descriptions more precise regarding car features and components?\")\n",
    "print(\"\\n3. Caption relevance:\")\n",
    "print(\"   - Do the captions focus on aspects of the image that are important for Mercedes?\")\n",
    "print(\"   - Are industry-relevant details highlighted?\")\n",
    "print(\"\\n4. Overall improvement areas:\")\n",
    "print(\"   - Where does the fine-tuned model excel compared to the base model?\")\n",
    "print(\"   - What areas still need improvement in the fine-tuned model?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze metrics to determine if fine-tuning improved performance\n",
    "improvement = {}\n",
    "for metric in avg_df.columns:\n",
    "    base_score = avg_df.loc['Base Model', metric]\n",
    "    ft_score = avg_df.loc['Fine-tuned Model', metric]\n",
    "    pct_change = ((ft_score - base_score) / base_score) * 100 if base_score > 0 else float('inf')\n",
    "    improvement[metric] = pct_change\n",
    "\n",
    "improvement_df = pd.DataFrame([improvement], index=['Percentage Improvement'])\n",
    "print(\"Performance Improvement Analysis:\")\n",
    "print(\"-\" * 60)\n",
    "print(improvement_df)\n",
    "\n",
    "# Identify strength and weakness areas\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"1. Model Performance Comparison:\")\n",
    "print(\"   This notebook has compared the base BLIP model with the fine-tuned BLIP model\")\n",
    "print(\"   specifically trained on Mercedes car images.\")\n",
    "print(\"\\n2. Quantitative Improvements:\")\n",
    "for metric, pct in improvement.items():\n",
    "    direction = \"improved\" if pct > 0 else \"declined\"\n",
    "    print(f\"   - {metric}: {abs(pct):.2f}% {direction}\")\n",
    "print(\"\\n3. Qualitative Observations:\")\n",
    "print(\"   - Strengths of fine-tuned model: [fill in based on observations]\")\n",
    "print(\"   - Areas for further improvement: [fill in based on observations]\")\n",
    "print(\"\\n4. Recommendations:\")\n",
    "print(\"   - If improvements are significant: Use the fine-tuned model for production\")\n",
    "print(\"   - If improvements are mixed: Consider additional fine-tuning with more specific data\")\n",
    "print(\"   - If improvements are marginal: Analyze where the fine-tuning process could be enhanced\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 