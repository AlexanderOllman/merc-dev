{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow to BentoML Model Conversion\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load a fine-tuned model from MLflow\n",
    "2. Convert it to a BentoML model\n",
    "3. Save it back to MLflow for deployment with a Bento server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install mlflow bentoml torch torchvision transformers pillow boto3 matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import bentoml\n",
    "from mlflow.tracking import MlflowClient\n",
    "import torch\n",
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import traceback\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "print(f\"Using device: {device} with dtype: {model_dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the fine-tuned model from MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh token if needed\n",
    "%update_token\n",
    "\n",
    "# Get the latest version of the registered BLIP2 model\n",
    "client = MlflowClient()\n",
    "registered_model_name = \"blip_ft_production\"  # The name of your fine-tuned model in MLflow\n",
    "\n",
    "# Get the latest version\n",
    "latest_versions = client.get_latest_versions(registered_model_name)\n",
    "latest_version = None\n",
    "\n",
    "for version in latest_versions:\n",
    "    if version.current_stage == \"Production\":\n",
    "        latest_version = version\n",
    "        break\n",
    "    \n",
    "if latest_version is None and len(latest_versions) > 0:\n",
    "    latest_version = latest_versions[0]\n",
    "\n",
    "if latest_version is None:\n",
    "    raise ValueError(f\"No versions found for model {registered_model_name}\")\n",
    "\n",
    "print(f\"Using model version: {latest_version.version}, stage: {latest_version.current_stage}\")\n",
    "\n",
    "# Get the model's artifact URI\n",
    "model_uri = f\"models:/{registered_model_name}/{latest_version.version}\"\n",
    "print(f\"Model URI: {model_uri}\")\n",
    "\n",
    "# Load the model from MLflow\n",
    "feature_extractor = mlflow.pytorch.load_model(model_uri, map_location=device)\n",
    "feature_extractor = feature_extractor.to(model_dtype)\n",
    "feature_extractor.eval()\n",
    "print(f\"Model loaded successfully with dtype: {next(feature_extractor.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define preprocessing and prediction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom preprocessing function based on standard BLIP2 preprocessing\n",
    "def preprocess_image(image, size=224):\n",
    "    # BLIP2 standard preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), \n",
    "                             (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])\n",
    "    \n",
    "    # Convert to RGB if it's not already\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "        \n",
    "    # Apply transformations\n",
    "    tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return tensor\n",
    "\n",
    "# Define a prediction function for BentoML\n",
    "def predict(image):\n",
    "    processed_image = preprocess_image(image).to(device).to(model_dtype)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = feature_extractor(processed_image)\n",
    "    \n",
    "    return features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the model with a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure S3 connection\n",
    "object_storage_service_name = \"source-images-service\"\n",
    "object_storage_namespace = \".ezdata-system\"\n",
    "resource_type = \".svc\"\n",
    "domain = \".cluster.local\"\n",
    "object_storage_port = \"30000\"\n",
    "\n",
    "s3_endpoint_url = f\"http://{object_storage_service_name}{object_storage_namespace}{resource_type}{domain}:{object_storage_port}\"\n",
    "print(f\"S3 endpoint URL: {s3_endpoint_url}\")\n",
    "\n",
    "# Create S3 clients\n",
    "s3_client = boto3.client('s3', endpoint_url=s3_endpoint_url)\n",
    "s3_resource = boto3.resource('s3', endpoint_url=s3_endpoint_url)\n",
    "\n",
    "# Set bucket name\n",
    "bucket_name = \"poc-mercedes-gp\"\n",
    "\n",
    "# Load the dataset JSON file\n",
    "file_key = \"training/training_dataset.json\"\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "content = response[\"Body\"].read().decode(\"utf-8\")\n",
    "dataset = json.loads(content)\n",
    "\n",
    "# Get the first image\n",
    "first_image_key = dataset[0]['s3_key']\n",
    "print(f\"First image S3 key: {first_image_key}\")\n",
    "\n",
    "# Download the image\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=first_image_key)\n",
    "image_data = response['Body'].read()\n",
    "\n",
    "# Convert to PIL Image\n",
    "image = Image.open(BytesIO(image_data))\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Image: {first_image_key.split('/')[-1]}\\nGround Truth: {dataset[0]['text']}\")\n",
    "plt.show()\n",
    "\n",
    "# Test the model\n",
    "features = predict(image)\n",
    "print(f\"Feature shape: {features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a BentoML model wrapper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlipFeatureExtractor(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), \n",
    "                                 (0.26862954, 0.26130258, 0.27577711))\n",
    "        ])\n",
    "    \n",
    "    def forward(self, image):\n",
    "        if isinstance(image, np.ndarray):\n",
    "            image = Image.fromarray(image.astype('uint8'))\n",
    "        \n",
    "        if isinstance(image, Image.Image):\n",
    "            # Convert to RGB if it's not already\n",
    "            if image.mode != \"RGB\":\n",
    "                image = image.convert(\"RGB\")\n",
    "            \n",
    "            # Apply transformations\n",
    "            image = self.transform(image).unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        if isinstance(image, torch.Tensor):\n",
    "            # Ensure the image is on the right device and dtype\n",
    "            device = next(self.model.parameters()).device\n",
    "            dtype = next(self.model.parameters()).dtype\n",
    "            image = image.to(device).to(dtype)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = self.model(image)\n",
    "        \n",
    "        return features\n",
    "\n",
    "# Create the BentoML model wrapper\n",
    "blip_wrapper = BlipFeatureExtractor(feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save the model to BentoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to BentoML\n",
    "bentoml_model_name = \"blip_feature_extractor\"\n",
    "bentoml_model_version = \"v1\"\n",
    "\n",
    "# Get model metadata from MLflow\n",
    "run = mlflow.tracking.MlflowClient().get_run(latest_version.run_id)\n",
    "mlflow_metadata = run.data.params if run.data.params else {}\n",
    "\n",
    "# Add additional metadata\n",
    "metadata = {\n",
    "    \"framework\": \"pytorch\",\n",
    "    \"model_type\": \"BLIP2\",\n",
    "    \"mlflow_model_name\": registered_model_name,\n",
    "    \"mlflow_model_version\": latest_version.version,\n",
    "    **mlflow_metadata\n",
    "}\n",
    "\n",
    "# Save the model using pickle approach to handle TorchScript models\n",
    "import cloudpickle\n",
    "\n",
    "# Save the model to a temporary file using cloudpickle\n",
    "model_file = \"blip_model.pkl\"\n",
    "with open(model_file, \"wb\") as f:\n",
    "    cloudpickle.dump(blip_wrapper, f)\n",
    "\n",
    "# Use BentoML's picklable_model module instead of pytorch\n",
    "bentoml_model = bentoml.picklable_model.save_model(\n",
    "    name=bentoml_model_name,\n",
    "    path=model_file,\n",
    "    signatures={\n",
    "        \"__call__\": {\n",
    "            \"batchable\": True,\n",
    "        },\n",
    "    },\n",
    "    labels={\n",
    "        \"framework\": \"pytorch\",\n",
    "        \"model_type\": \"BLIP2\",\n",
    "    },\n",
    "    metadata=metadata,\n",
    ")\n",
    "\n",
    "# Remove the temporary file\n",
    "!rm {model_file}\n",
    "\n",
    "print(f\"Model saved to BentoML: {bentoml_model.tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the BentoML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from BentoML\n",
    "loaded_model = bentoml.picklable_model.load_model(bentoml_model.tag)\n",
    "\n",
    "# Test the loaded model\n",
    "processed_image = preprocess_image(image).to(device).to(model_dtype)\n",
    "with torch.no_grad():\n",
    "    bentoml_features = loaded_model(processed_image)\n",
    "\n",
    "print(f\"BentoML model features shape: {bentoml_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create a BentoML service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile service.py\n",
    "\n",
    "import bentoml\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Define the service\n",
    "runner = bentoml.picklable_model.get(\"blip_feature_extractor:latest\").to_runner()\n",
    "\n",
    "svc = bentoml.Service(\"blip_feature_extractor\", runners=[runner])\n",
    "\n",
    "@svc.api(input=bentoml.io.Image(), output=bentoml.io.NumpyNdarray())\n",
    "def extract_features(img):\n",
    "    # The preprocessing is now handled in the model wrapper\n",
    "    features = runner.run(img)\n",
    "    return features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export the BentoML model to MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new MLflow run\n",
    "mlflow.set_experiment(\"BentoML-BLIP2\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"bentoml_conversion\") as run:\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"bentoml_model_tag\", bentoml_model.tag)\n",
    "    mlflow.log_param(\"original_mlflow_model\", f\"{registered_model_name}/{latest_version.version}\")\n",
    "    \n",
    "    # Create a directory for the BentoML model export\n",
    "    export_dir = \"bentoml_export\"\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    \n",
    "    # Export BentoML model metadata to a file\n",
    "    with open(f\"{export_dir}/bentoml_metadata.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"tag\": bentoml_model.tag,\n",
    "            \"creation_time\": bentoml_model.creation_time.isoformat(),\n",
    "            \"labels\": bentoml_model.labels,\n",
    "            \"metadata\": bentoml_model.metadata,\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    # Log metadata file as an artifact\n",
    "    mlflow.log_artifact(f\"{export_dir}/bentoml_metadata.json\")\n",
    "    \n",
    "    # Export the service.py file\n",
    "    mlflow.log_artifact(\"service.py\")\n",
    "    \n",
    "    # Create and log bentofile.yaml\n",
    "    bentofile_content = f\"\"\"\n",
    "service: \"service:svc\"\n",
    "description: \"BLIP2 Feature Extractor Service\"\n",
    "labels:\n",
    "  model: \"blip2\"\n",
    "  framework: \"pytorch\"\n",
    "include:\n",
    "- \"service.py\"\n",
    "python:\n",
    "  packages:\n",
    "  - torch\n",
    "  - pillow\n",
    "  - numpy\n",
    "  - cloudpickle\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(f\"{export_dir}/bentofile.yaml\", \"w\") as f:\n",
    "        f.write(bentofile_content)\n",
    "    \n",
    "    mlflow.log_artifact(f\"{export_dir}/bentofile.yaml\")\n",
    "    \n",
    "    # Save the model file for MLflow logging\n",
    "    model_file = \"bentoml_blip_model.pkl\"\n",
    "    with open(model_file, \"wb\") as f:\n",
    "        cloudpickle.dump(loaded_model, f)\n",
    "    \n",
    "    # Log the model file as a generic artifact instead of using pytorch-specific logging\n",
    "    mlflow.log_artifact(model_file, \"bentoml_blip_model\")\n",
    "    \n",
    "    # Register the model using generic artifacts (optional)\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    model_uri = f\"runs:/{run.info.run_id}/bentoml_blip_model\"\n",
    "    client.create_model_version(\n",
    "        name=\"blip_bentoml_production\",\n",
    "        source=model_uri,\n",
    "        run_id=run.info.run_id\n",
    "    )\n",
    "    \n",
    "    # Clean up the temporary model file\n",
    "    !rm {model_file}\n",
    "    \n",
    "    # Get the run ID for later use\n",
    "    run_id = run.info.run_id\n",
    "    \n",
    "    print(f\"MLflow run ID: {run_id}\")\n",
    "    print(f\"MLflow experiment URL: {mlflow.get_artifact_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create a Bento from the BentoML model and add it to MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a bento\n",
    "!bentoml build\n",
    "\n",
    "# List available bentos\n",
    "bento_list_output = !bentoml list\n",
    "print(bento_list_output)\n",
    "\n",
    "# Get the latest bento tag (assuming the first one is the latest)\n",
    "latest_bento = None\n",
    "for line in bento_list_output:\n",
    "    if \"blip_feature_extractor:\" in line:\n",
    "        latest_bento = line.split()[0]\n",
    "        break\n",
    "\n",
    "if latest_bento:\n",
    "    print(f\"Latest bento: {latest_bento}\")\n",
    "    \n",
    "    # Export the bento to a local directory\n",
    "    bento_export_dir = \"bento_export\"\n",
    "    os.makedirs(bento_export_dir, exist_ok=True)\n",
    "    \n",
    "    # Export the bento to a .bento file\n",
    "    bento_path = f\"{bento_export_dir}/{latest_bento.replace(':', '_')}.bento\"\n",
    "    !bentoml export {latest_bento} {bento_path}\n",
    "    \n",
    "    print(f\"Exported bento to: {bento_path}\")\n",
    "    \n",
    "    # Add the .bento file to MLflow\n",
    "    with mlflow.start_run(run_id=run_id):\n",
    "        mlflow.log_artifact(bento_path)\n",
    "        print(f\"Added {bento_path} to MLflow run: {run_id}\")\n",
    "        \n",
    "        # Log the bento tag as a parameter\n",
    "        mlflow.log_param(\"bento_tag\", latest_bento)\n",
    "        \n",
    "        # Create a README with instructions on how to use the .bento file\n",
    "        readme_content = f\"\"\"\n",
    "# BLIP2 Feature Extractor Bento\n",
    "\n",
    "This is a BentoML package for the BLIP2 feature extractor model.\n",
    "\n",
    "## Using the Bento\n",
    "\n",
    "To use this Bento, you need to import it into a BentoML environment:\n",
    "\n",
    "```bash\n",
    "# Import the Bento\n",
    "bentoml import {latest_bento.replace(':', '_')}.bento\n",
    "\n",
    "# Serve the model\n",
    "bentoml serve {latest_bento}\n",
    "```\n",
    "\n",
    "## Deploying with BentoML\n",
    "\n",
    "To deploy this model with BentoML, you can containerize it:\n",
    "\n",
    "```bash\n",
    "bentoml containerize {latest_bento}\n",
    "```\n",
    "\n",
    "Or deploy it directly to BentoCloud:\n",
    "\n",
    "```bash\n",
    "bentoml deploy {latest_bento}\n",
    "```\n",
    "\"\"\"\n",
           "\n",
    "        readme_path = f\"{bento_export_dir}/README.md\"\n",
    "        with open(readme_path, \"w\") as f:\n",
    "            f.write(readme_content)\n",
    "        \n",
    "        mlflow.log_artifact(readme_path)\n",
    "else:\n",
    "    print(\"No bento found. Please check the build process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test invoking the Bento service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serve the model in the background\n",
    "!bentoml serve service:svc &\n",
    "\n",
    "# Allow some time for the service to start\n",
    "import time\n",
    "time.sleep(5)\n",
    "\n",
    "# Test the API with multiple images from the dataset\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test with multiple images from the dataset\n",
    "test_indices = [0, 1, 2]  # We'll try the first three images from the dataset\n",
    "\n",
    "for idx in test_indices:\n",
    "    if idx < len(dataset):\n",
    "        print(f\"\\nProcessing test image {idx}:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Get image from S3\n",
    "        image_key = dataset[idx]['s3_key']\n",
    "        print(f\"Image S3 key: {image_key}\")\n",
    "        \n",
    "        try:\n",
    "            # Download image from S3\n",
    "            response = s3_client.get_object(Bucket=bucket_name, Key=image_key)\n",
    "            image_data = response['Body'].read()\n",
    "            test_image = Image.open(BytesIO(image_data))\n",
    "            \n",
    "            # Display image with ground truth caption\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.imshow(test_image)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"Image: {image_key.split('/')[-1]}\\nGround Truth: {dataset[idx]['text']}\")\n",
    "            plt.show()\n",
    "            \n",
    "            # Save the image to a temporary file\n",
    "            temp_path = f\"temp_image_{idx}.jpg\"\n",
    "            test_image.save(temp_path)\n",
    "            \n",
    "            # Send the image to the API\n",
    "            with open(temp_path, \"rb\") as f:\n",
    "                response = requests.post(\n",
    "                    \"http://localhost:3000/extract_features\",\n",
    "                    files={\"img\": (f\"image_{idx}.jpg\", f, \"image/jpeg\")}\n",
    "                )\n",
    "            \n",
    "            # Process the response\n",
    "            if response.status_code == 200:\n",
    "                features = np.array(response.json())\n",
    "                print(f\"API response successful!\")\n",
    "                print(f\"Feature vector shape: {features.shape}\")\n",
    "                \n",
    "                # Display the first few values of the feature vector\n",
    "                print(f\"First 5 feature values: {features.flatten()[:5]}\")\n",
    "            else:\n",
    "                print(f\"Error: {response.status_code}\")\n",
    "                print(response.text)\n",
    "                \n",
    "            # Clean up the temporary file\n",
    "            !rm {temp_path}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {idx}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "# Stop the service\n",
    "!pkill -f \"bentoml serve\"\n",
    "print(\"\\nBentoML service stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully:\n",
    "1. Loaded the fine-tuned BLIP2 model from MLflow\n",
    "2. Converted it to a BentoML model\n",
    "3. Created a BentoML service to serve the model\n",
    "4. Saved the BentoML model and service definition back to MLflow\n",
    "5. Built and tested a Bento service\n",
    "\n",
    "The model is now ready for deployment with BentoML, which provides additional capabilities like:\n",
    "- Adaptive batching\n",
    "- Request queueing\n",
    "- Monitoring and metrics\n",
    "- Containerization and Kubernetes deployment\n",
    "- API management features\n",
    "\n",
    "To deploy the Bento service to production, you can use BentoML's deployment options, such as:\n",
    "```bash\n",
    "bentoml containerize blip_feature_extractor:latest\n",
    "```\n",
    "\n",
    "Or use BentoML's Kubernetes operator, Yatai, for orchestrated deployments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 