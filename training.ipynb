{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c32e733-a5c1-4f72-a932-42673eef1930",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar  3 08:16:30 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA L40S                    On  |   00000000:47:00.0 Off |                    0 |\n",
      "| N/A   30C    P8             34W /  350W |       1MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37bc88d7-d83b-49e6-a168-ffa0bd13798d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch)\n",
      "  Using cached triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
      "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "Using cached torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
      "Using cached filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: triton, nvidia-cusparselt-cu12, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "Successfully installed filelock-3.17.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 torchvision-0.21.0 triton-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a8c82de-591e-4393-9825-1276c5aa3ca6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting salesforce-lavis\n",
      "  Using cached salesforce_lavis-1.0.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting contexttimer (from salesforce-lavis)\n",
      "  Using cached contexttimer-0.3.3-py3-none-any.whl\n",
      "Collecting decord (from salesforce-lavis)\n",
      "  Using cached decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n",
      "Collecting einops>=0.4.1 (from salesforce-lavis)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fairscale==0.4.4 (from salesforce-lavis)\n",
      "  Using cached fairscale-0.4.4-py3-none-any.whl\n",
      "Collecting ftfy (from salesforce-lavis)\n",
      "  Using cached ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting iopath (from salesforce-lavis)\n",
      "  Using cached iopath-0.1.10-py3-none-any.whl\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.11/site-packages (from salesforce-lavis) (8.22.2)\n",
      "Collecting omegaconf (from salesforce-lavis)\n",
      "  Using cached omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting opencv-python-headless==4.5.5.64 (from salesforce-lavis)\n",
      "  Using cached opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting opendatasets (from salesforce-lavis)\n",
      "  Using cached opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from salesforce-lavis) (24.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from salesforce-lavis) (2.1.4)\n",
      "Collecting plotly (from salesforce-lavis)\n",
      "  Using cached plotly-6.0.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pre-commit (from salesforce-lavis)\n",
      "  Using cached pre_commit-4.1.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting pycocoevalcap (from salesforce-lavis)\n",
      "  Using cached pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pycocotools (from salesforce-lavis)\n",
      "  Using cached pycocotools-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting python-magic (from salesforce-lavis)\n",
      "  Using cached python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.11/site-packages (from salesforce-lavis) (0.22.0)\n",
      "Collecting sentencepiece (from salesforce-lavis)\n",
      "  Using cached sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting spacy (from salesforce-lavis)\n",
      "  Using cached spacy-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting streamlit (from salesforce-lavis)\n",
      "  Using cached streamlit-1.42.2-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting timm==0.4.12 (from salesforce-lavis)\n",
      "  Using cached timm-0.4.12-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from salesforce-lavis) (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (from salesforce-lavis) (0.21.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from salesforce-lavis) (4.66.6)\n",
      "Collecting transformers<4.27,>=4.25.0 (from salesforce-lavis)\n",
      "  Using cached transformers-4.26.1-py3-none-any.whl.metadata (100 kB)\n",
      "Collecting webdataset (from salesforce-lavis)\n",
      "  Using cached webdataset-0.2.111-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.11/site-packages (from salesforce-lavis) (0.44.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.11/site-packages (from opencv-python-headless==4.5.5.64->salesforce-lavis) (1.26.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->salesforce-lavis) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->salesforce-lavis) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->salesforce-lavis) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->salesforce-lavis) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->salesforce-lavis) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->salesforce-lavis) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->salesforce-lavis) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->salesforce-lavis) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->salesforce-lavis) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->salesforce-lavis) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->salesforce-lavis) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->salesforce-lavis) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->salesforce-lavis) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->salesforce-lavis) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->salesforce-lavis) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->salesforce-lavis) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->salesforce-lavis) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->salesforce-lavis) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->salesforce-lavis) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->salesforce-lavis) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.10.0->salesforce-lavis) (1.3.0)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers<4.27,>=4.25.0->salesforce-lavis)\n",
      "  Using cached huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers<4.27,>=4.25.0->salesforce-lavis) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers<4.27,>=4.25.0->salesforce-lavis)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers<4.27,>=4.25.0->salesforce-lavis) (2.32.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<4.27,>=4.25.0->salesforce-lavis)\n",
      "  Using cached tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from ftfy->salesforce-lavis) (0.2.13)\n",
      "Collecting portalocker (from iopath->salesforce-lavis)\n",
      "  Using cached portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from ipython->salesforce-lavis) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.11/site-packages (from ipython->salesforce-lavis) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.11/site-packages (from ipython->salesforce-lavis) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.11/site-packages (from ipython->salesforce-lavis) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from ipython->salesforce-lavis) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.11/site-packages (from ipython->salesforce-lavis) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /opt/conda/lib/python3.11/site-packages (from ipython->salesforce-lavis) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.11/site-packages (from ipython->salesforce-lavis) (4.9.0)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->salesforce-lavis)\n",
      "  Using cached antlr4_python3_runtime-4.9.3-py3-none-any.whl\n",
      "Collecting kaggle (from opendatasets->salesforce-lavis)\n",
      "  Using cached kaggle-1.6.17-py3-none-any.whl\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from opendatasets->salesforce-lavis) (8.1.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->salesforce-lavis) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->salesforce-lavis) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas->salesforce-lavis) (2024.2)\n",
      "Collecting narwhals>=1.15.1 (from plotly->salesforce-lavis)\n",
      "  Using cached narwhals-1.28.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting cfgv>=2.0.0 (from pre-commit->salesforce-lavis)\n",
      "  Using cached cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting identify>=1.0.0 (from pre-commit->salesforce-lavis)\n",
      "  Using cached identify-2.6.8-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting nodeenv>=0.11.1 (from pre-commit->salesforce-lavis)\n",
      "  Using cached nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Collecting virtualenv>=20.10.0 (from pre-commit->salesforce-lavis)\n",
      "  Using cached virtualenv-20.29.2-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from pycocotools->salesforce-lavis) (3.8.4)\n",
      "Requirement already satisfied: scipy>=1.8 in /opt/conda/lib/python3.11/site-packages (from scikit-image->salesforce-lavis) (1.11.3)\n",
      "Requirement already satisfied: pillow>=9.0.1 in /opt/conda/lib/python3.11/site-packages (from scikit-image->salesforce-lavis) (11.0.0)\n",
      "Requirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.11/site-packages (from scikit-image->salesforce-lavis) (2.36.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.11/site-packages (from scikit-image->salesforce-lavis) (2024.9.20)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.11/site-packages (from scikit-image->salesforce-lavis) (0.4)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy->salesforce-lavis)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy->salesforce-lavis)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy->salesforce-lavis)\n",
      "  Using cached murmurhash-1.0.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy->salesforce-lavis)\n",
      "  Using cached cymem-2.0.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy->salesforce-lavis)\n",
      "  Using cached preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy->salesforce-lavis)\n",
      "  Using cached thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy->salesforce-lavis)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy->salesforce-lavis)\n",
      "  Using cached srsly-2.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy->salesforce-lavis)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy->salesforce-lavis)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy->salesforce-lavis)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.11/site-packages (from spacy->salesforce-lavis) (2.9.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from spacy->salesforce-lavis) (75.3.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy->salesforce-lavis)\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /opt/conda/lib/python3.11/site-packages (from streamlit->salesforce-lavis) (5.4.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from streamlit->salesforce-lavis) (1.8.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /opt/conda/lib/python3.11/site-packages (from streamlit->salesforce-lavis) (5.5.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /opt/conda/lib/python3.11/site-packages (from streamlit->salesforce-lavis) (4.25.5)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /opt/conda/lib/python3.11/site-packages (from streamlit->salesforce-lavis) (11.0.0)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit->salesforce-lavis)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from streamlit->salesforce-lavis) (8.5.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/conda/lib/python3.11/site-packages (from streamlit->salesforce-lavis) (0.10.2)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit->salesforce-lavis)\n",
      "  Using cached watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/conda/lib/python3.11/site-packages (from streamlit->salesforce-lavis) (3.1.43)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit->salesforce-lavis)\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /opt/conda/lib/python3.11/site-packages (from streamlit->salesforce-lavis) (6.4.1)\n",
      "Collecting braceexpand (from webdataset->salesforce-lavis)\n",
      "  Using cached braceexpand-0.1.7-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit->salesforce-lavis) (4.23.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.11/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->salesforce-lavis) (4.0.11)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.11/site-packages (from jedi>=0.16->ipython->salesforce-lavis) (0.8.4)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy->salesforce-lavis)\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (3.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.11/site-packages (from pexpect>4.3->ipython->salesforce-lavis) (0.7.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->salesforce-lavis) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->salesforce-lavis) (2.23.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->salesforce-lavis) (3.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->salesforce-lavis) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers<4.27,>=4.25.0->salesforce-lavis) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers<4.27,>=4.25.0->salesforce-lavis) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers<4.27,>=4.25.0->salesforce-lavis) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers<4.27,>=4.25.0->salesforce-lavis) (2024.8.30)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14,>=10.14.0->streamlit->salesforce-lavis)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy->salesforce-lavis)\n",
      "  Using cached blis-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy->salesforce-lavis)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy->salesforce-lavis)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->salesforce-lavis)\n",
      "  Using cached distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in /opt/conda/lib/python3.11/site-packages (from virtualenv>=20.10.0->pre-commit->salesforce-lavis) (4.3.6)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy->salesforce-lavis)\n",
      "  Using cached cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy->salesforce-lavis)\n",
      "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting python-slugify (from kaggle->opendatasets->salesforce-lavis)\n",
      "  Using cached python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.11/site-packages (from kaggle->opendatasets->salesforce-lavis) (6.2.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->salesforce-lavis) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->salesforce-lavis) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->salesforce-lavis) (0.2.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->salesforce-lavis) (5.0.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->salesforce-lavis) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->salesforce-lavis) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->salesforce-lavis) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->salesforce-lavis) (0.21.0)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->salesforce-lavis)\n",
      "  Using cached marisa_trie-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->salesforce-lavis)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->salesforce-lavis) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.11/site-packages (from bleach->kaggle->opendatasets->salesforce-lavis) (0.5.1)\n",
      "Collecting text-unidecode>=1.3 (from python-slugify->kaggle->opendatasets->salesforce-lavis)\n",
      "  Using cached text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Using cached salesforce_lavis-1.0.2-py3-none-any.whl (1.8 MB)\n",
      "Using cached opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.8 MB)\n",
      "Using cached timm-0.4.12-py3-none-any.whl (376 kB)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Using cached transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "Using cached decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
      "Using cached ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Using cached opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
      "Using cached plotly-6.0.0-py3-none-any.whl (14.8 MB)\n",
      "Using cached pre_commit-4.1.0-py2.py3-none-any.whl (220 kB)\n",
      "Using cached pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
      "Using cached pycocotools-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (458 kB)\n",
      "Using cached python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Using cached sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached spacy-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.6 MB)\n",
      "Using cached streamlit-1.42.2-py2.py3-none-any.whl (9.6 MB)\n",
      "Using cached webdataset-0.2.111-py3-none-any.whl (85 kB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n",
      "Using cached cymem-2.0.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (218 kB)\n",
      "Using cached huggingface_hub-0.29.1-py3-none-any.whl (468 kB)\n",
      "Using cached identify-2.6.8-py2.py3-none-any.whl (99 kB)\n",
      "Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Using cached murmurhash-1.0.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (134 kB)\n",
      "Using cached narwhals-1.28.0-py3-none-any.whl (308 kB)\n",
      "Using cached nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
      "Using cached preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
      "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Using cached srsly-2.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Using cached thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
      "Using cached tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Using cached virtualenv-20.29.2-py3-none-any.whl (4.3 MB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
      "Using cached portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Using cached blis-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "Using cached cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
      "Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Using cached python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Using cached marisa_trie-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tokenizers, text-unidecode, sentencepiece, distlib, cymem, contexttimer, braceexpand, antlr4-python3-runtime, webdataset, watchdog, wasabi, virtualenv, spacy-loggers, spacy-legacy, smart-open, shellingham, regex, python-slugify, python-magic, portalocker, opencv-python-headless, omegaconf, nodeenv, narwhals, murmurhash, mdurl, marisa-trie, identify, ftfy, einops, decord, cloudpathlib, cfgv, catalogue, blis, srsly, pydeck, preshed, pre-commit, plotly, markdown-it-py, language-data, kaggle, iopath, huggingface-hub, transformers, rich, pycocotools, opendatasets, langcodes, confection, typer, thinc, pycocoevalcap, fairscale, weasel, timm, streamlit, spacy, salesforce-lavis\n",
      "  Attempting uninstall: narwhals\n",
      "    Found existing installation: narwhals 1.13.2\n",
      "    Uninstalling narwhals-1.13.2:\n",
      "      Successfully uninstalled narwhals-1.13.2\n",
      "Successfully installed antlr4-python3-runtime-4.9.3 blis-1.2.0 braceexpand-0.1.7 catalogue-2.0.10 cfgv-3.4.0 cloudpathlib-0.20.0 confection-0.1.5 contexttimer-0.3.3 cymem-2.0.11 decord-0.6.0 distlib-0.3.9 einops-0.8.1 fairscale-0.4.4 ftfy-6.3.1 huggingface-hub-0.29.1 identify-2.6.8 iopath-0.1.10 kaggle-1.6.17 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.12 narwhals-1.28.0 nodeenv-1.9.1 omegaconf-2.3.0 opencv-python-headless-4.5.5.64 opendatasets-0.1.22 plotly-6.0.0 portalocker-3.1.1 pre-commit-4.1.0 preshed-3.0.9 pycocoevalcap-1.2 pycocotools-2.0.8 pydeck-0.9.1 python-magic-0.4.27 python-slugify-8.0.4 regex-2024.11.6 rich-13.9.4 salesforce-lavis-1.0.2 sentencepiece-0.2.0 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 streamlit-1.42.2 text-unidecode-1.3 thinc-8.3.4 timm-0.4.12 tokenizers-0.13.3 transformers-4.26.1 typer-0.15.2 virtualenv-20.29.2 wasabi-1.1.3 watchdog-6.0.0 weasel-0.4.1 webdataset-0.2.111\n"
     ]
    }
   ],
   "source": [
    "!pip install salesforce-lavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26ecb214-6136-4a56-826f-44a1813f7850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore\n",
      "/opt/conda/lib/python3.11/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torchvision.transforms import Compose\n",
    "from PIL import Image\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from lavis.models import load_model_and_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97a5c52e-7ba9-4460-9d96-bcbfb4aa656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"blip2_feature_extractor\"\n",
    "model_type = \"pretrain\"  # Use \"pretrained\" as a starting point\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "lr = 1e-5\n",
    "# log_dir = \"logs/run_test\"  # TensorBoard log directory\n",
    "save_model_path = \"blip2_finetuned_v2.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bd9de01-9bf1-4367-b4f8-913879b449db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "freeze vision encoder\n",
      "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained.pth\n"
     ]
    }
   ],
   "source": [
    "# Load the model and preprocessors\n",
    "model, vis_processors, txt_processors = load_model_and_preprocess(\n",
    "    model_name, model_type=model_type, is_eval=False\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3165461-bff5-48b6-b0c6-b6fd2a4629a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d5cb58e-2f6a-4a74-a10d-ed2b0e24bb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://source-images-service.ezdata-system.svc.cluster.local:30000\n"
     ]
    }
   ],
   "source": [
    "object_storage_service_name = \"source-images-service\"\n",
    "object_storage_namespace = \".ezdata-system\"\n",
    "resource_type = \".svc\"\n",
    "domain = \".cluster.local\"\n",
    "object_storage_port = \"30000\"\n",
    "\n",
    "\n",
    "s3_endpoint_url = f\"http://{object_storage_service_name}{object_storage_namespace}{resource_type}{domain}:{object_storage_port}\"\n",
    "print(s3_endpoint_url)\n",
    "\n",
    "\n",
    "s3_client = boto3.client('s3', endpoint_url=s3_endpoint_url)\n",
    "s3_resource = boto3.resource('s3', endpoint_url=s3_endpoint_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6fb706a6-21b5-4007-bac7-1bb87cbb834e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bucket = s3_resource.Bucket('poc-mercedes-gp')\n",
    "\n",
    "training_list = []\n",
    "for bucket_object in bucket.objects.all():\n",
    "    if bucket_object.key.startswith('training/'):\n",
    "        training_list.append(bucket_object.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bcb034d8-0996-410b-abde-5d76d7356911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "827"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d79b93b8-98e7-4885-a330-55a34d86d007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1481"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "497b0017-4c80-425a-98f9-db8d4bacc0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "bucket_name = \"poc-mercedes-gp\"\n",
    "file_key = \"training/training_dataset.json\"\n",
    "\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "content = response[\"Body\"].read().decode(\"utf-8\")  # Convert to string if it's a text file\n",
    "\n",
    "dataset = json.loads(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "223fb668-b2e6-41f4-b826-ff3ee42a6bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'s3_key': 'training/03_05Bah_Sunday_Alfa Romeo_013.JPG',\n",
       " 'text': 'A photo of a floor'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3d6231c-710d-4ffb-971f-360d27fdacf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_RedBull1199.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/RedBullTyresAfterFP10084.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A3625.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE57046.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE58213.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE58454.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE53636.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE51825.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_J5A0265.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/RedBullTyresAfterFP10080.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_J5A3131.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_J5A8664.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A7031.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE53628.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A1470 2.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/AstonMartinTyresAfterFP10160.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/AstonMartinTyresAfterFP10163.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/RedBullTyresAfterFP10086.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_J5A3702.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A3152.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_RedBull1180.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE51827.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A4787.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A7499.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_Ferrari0844.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A0327.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A1366.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE52235.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE57615.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE54801.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_G5A0596.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE59680.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/McLarenTyresAfterFP10136.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A1768.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE57881.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_Ferrari0783.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A3179.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE51822.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE57067.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A4799.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE58206.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE52247.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A0335.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A7818.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_J5A3139.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A6966.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_J5A8051.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_J5A2756.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_J5A5499.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A1771.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE55596.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A2586.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_McLaren1047.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE58219.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE59441.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_J5A5500.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_J5A0820.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A5417.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_J5A0266.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A2854.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A7545.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A1469 2.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE50376.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE57870.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE57036.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A1759.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE52957.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE55584.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A3523.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE54781.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/RedBullTyresAfterFP10094.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A1616.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE50688.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE58450.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A1841.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE52982.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A7543.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE53982.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A5268.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A5418.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE54983.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE52267.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE53622.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE51858.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/McLarenTyresAfterFP10134.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A9954.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_J5A0833.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A3489.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE52620.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A7262.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/McLarenTyresAfterFP10010.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_J5A5484.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_J5A3719.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE53631.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE57410.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A0359.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE52036.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_J5A0831.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A0337.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A7188.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE52047.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/McLarenTyresAfterFP10015.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A1295.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE54793.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_J5A3628.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_J5A3137.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE58444.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_J5A1811.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_J5A8116.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A0214.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE57265.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE54196.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/SNE52848.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_Ferrari0838.JPG\n",
      "Exception An error occurred (404) when calling the HeadObject operation: Not Found for training/_V6A7604.JPG\n"
     ]
    }
   ],
   "source": [
    "# Copy all the pictures locally\n",
    "from pathlib import Path\n",
    "\n",
    "Path('training_images').mkdir(exist_ok=True)\n",
    "checked_dataset = []\n",
    "\n",
    "for item in dataset:\n",
    "    try:\n",
    "        s3_client.download_file(bucket_name, item['s3_key'], f'training_images/{item[\"s3_key\"].split(\"/\")[-1]}')\n",
    "        checked_dataset.append(item)\n",
    "    except Exception as e:\n",
    "        print(f'Exception {e} for {item[\"s3_key\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "157a7369-f592-40f3-93fc-d6792a4ab036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1366    2488   49706\n"
     ]
    }
   ],
   "source": [
    "!ls training_images | wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f240ff8-cc7b-4b8b-8843-0af19c1022f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/user\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb0d633d-1d33-4ef4-ad97-2e8c77ba528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, vis_processors, txt_processors, s3_endpoint_url):\n",
    "        self.data = data\n",
    "        self.vis_processors = vis_processors\n",
    "        self.txt_processors = txt_processors\n",
    "\n",
    "        self.s3_client = boto3.client('s3', endpoint_url=s3_endpoint_url)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = os.path.join('training_images', self.data[idx][\"s3_key\"].split('/')[-1])\n",
    "        text = self.data[idx][\"text\"]\n",
    "        \n",
    "        # Open the image with PIL\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        image = self.vis_processors[\"train\"](image)\n",
    "        text = self.txt_processors[\"train\"](text)\n",
    "\n",
    "        return {\"image\": image, \"text\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a74402e-77e2-4592-9589-1e671d10ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "train_data = checked_dataset[:int(0.8 * len(checked_dataset))]\n",
    "val_data = checked_dataset[int(0.8 * len(checked_dataset)):]\n",
    "\n",
    "train_dataset = CustomDataset(train_data, vis_processors, txt_processors, s3_endpoint_url)\n",
    "val_dataset = CustomDataset(val_data, vis_processors, txt_processors, s3_endpoint_url)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a693b67-5aa0-4de2-8081-9891b51f5d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define contrastive loss\n",
    "def contrastive_loss(image_features, text_features):\n",
    "    image_features = F.normalize(image_features, dim=-1).mean(dim=1)\n",
    "    text_features = F.normalize(text_features, dim=-1)\n",
    "\n",
    "    logits = torch.mm(image_features, text_features.t())\n",
    "    labels = torch.arange(len(image_features)).to(logits.device)\n",
    "\n",
    "    loss_img_to_text = F.cross_entropy(logits, labels)\n",
    "    loss_text_to_img = F.cross_entropy(logits.t(), labels)\n",
    "\n",
    "    return (loss_img_to_text + loss_text_to_img) / 2\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6a621cf-70ce-42fc-86ed-d225dddc4662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import torch\n",
    "\n",
    "def get_token():\n",
    "    TOKEN_PATH = '/etc/secrets/ezua/.auth_token'\n",
    "    with open(TOKEN_PATH, 'r') as f:\n",
    "        auth_token = f.read()\n",
    "\n",
    "    os.environ['MLFLOW_TRACKING_TOKEN'] = auth_token\n",
    "    os.environ['AUTH_TOKEN'] = auth_token\n",
    "    return auth_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "024dc1fe-0f3c-40f3-aaa7-34fd7a0d1f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = get_token()\n",
    "\n",
    "os.environ['MLFLOW_TRACKING_URI'] = 'http://mlflow.mlflow.svc.cluster.local:5000'\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http://local-s3-service.ezdata-system.svc.cluster.local:30000'\n",
    "os.environ['MLFLOW_S3_IGNORE_TLS'] = 'true'\n",
    "os.environ['MLFLOW_TRACKING_INSECURE_TLS'] = 'true'\n",
    "os.environ['MLFLOW_TRACKING_TOKEN'] = token\n",
    "os.environ['AUTH_TOKEN'] = token\n",
    "\n",
    "mlflow.set_tracking_uri(os.environ[\"MLFLOW_TRACKING_URI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "65fc908b-ad17-415f-b635-5074cf86e797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://mlflow.fr2pcai169/6', creation_time=1741092490228, experiment_id='6', last_update_time=1741092490228, lifecycle_stage='active', name='BLIP 2 F1 finetuning', tags={}>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set MLflow experiment\n",
    "mlflow.set_experiment(\"BLIP 2 F1 finetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "70a4dfb7-1a12-4f5a-89a4-26cd3da7480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c59ea41-6fb1-4720-9e85-9746dc36e283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token successfully refreshed.\n",
      "Starting training\n",
      "token refreshed\n",
      "token refreshed\n",
      "token refreshed\n",
      "token refreshed\n",
      "token refreshed\n",
      "token refreshed\n",
      "token refreshed\n",
      "token refreshed\n",
      "token refreshed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/04 17:18:14 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token refreshed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/04 17:20:12 INFO mlflow.tracking._tracking_service.client:  View run crawling-owl-227 at: http://mlflow.mlflow.svc.cluster.local:5000/#/experiments/6/runs/d690584f793e493fbc4ca35cfbe59a78.\n",
      "2025/03/04 17:20:12 INFO mlflow.tracking._tracking_service.client:  View experiment at: http://mlflow.mlflow.svc.cluster.local:5000/#/experiments/6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 2.1145, Val Loss: 2.0954\n",
      "token refreshed\n",
      "token refreshed\n",
      "token refreshed\n",
      "token refreshed\n",
      "token refreshed\n",
      "token refreshed\n",
      "token refreshed\n",
      "token refreshed\n",
      "token refreshed\n",
      "token refreshed\n",
      "token refreshed\n"
     ]
    }
   ],
   "source": [
    "%update_token\n",
    "\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def refresh_token():\n",
    "    secret_file_path = \"/etc/secrets/ezua/.auth_token\"\n",
    "\n",
    "    with open(secret_file_path, \"r\") as file:\n",
    "    \ttoken = file.read().strip()\n",
    "    os.environ['MLFLOW_TRACKING_TOKEN']=token\n",
    "    print('token refreshed')\n",
    "\n",
    "def start_timer(interval=900):  # 900 seconds = 15 minutes\n",
    "    def loop():\n",
    "        while True:\n",
    "            refresh_token()\n",
    "            time.sleep(interval)\n",
    "\n",
    "    thread = threading.Thread(target=loop, daemon=True)\n",
    "    thread.start()\n",
    "\n",
    "start_timer()\n",
    "\n",
    "# Training Loop\n",
    "print('Starting training')\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.log_param(\"epoch\", epoch)\n",
    "        mlflow.log_param(\"learning_rate\", optimizer.param_groups[0][\"lr\"])\n",
    "        mlflow.log_param(\"batch_size\", train_loader.batch_size)\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            \n",
    "            images = batch[\"image\"].to(device)\n",
    "            texts = batch[\"text\"]\n",
    "            texts_tokenized = model.tokenizer(\n",
    "                    texts,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=35,\n",
    "                    return_tensors=\"pt\",\n",
    "                ).to(model.device)\n",
    "    \n",
    "            # Forward pass\n",
    "            image_patch_features, image_cls_features = model.forward_image(images)\n",
    "            text_features = model.forward_text(texts_tokenized)\n",
    "    \n",
    "            # Compute loss\n",
    "            loss = contrastive_loss(image_patch_features, text_features)\n",
    "    \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            train_loss += loss.item()\n",
    "    \n",
    "            # Log batch loss to MLflow\n",
    "            mlflow.log_metric(\"Loss/Train\", loss.item(), step=epoch * len(train_loader) + batch_idx)\n",
    "    \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch[\"image\"].to(device)\n",
    "                texts = batch[\"text\"]\n",
    "    \n",
    "                texts_tokenized = model.tokenizer(\n",
    "                        texts,\n",
    "                        padding=\"max_length\",\n",
    "                        truncation=True,\n",
    "                        max_length=35,\n",
    "                        return_tensors=\"pt\",\n",
    "                    ).to(model.device)\n",
    "    \n",
    "                image_patch_features, image_cls_features = model.forward_image(images)\n",
    "                text_features = model.forward_text(texts_tokenized)\n",
    "    \n",
    "                loss = contrastive_loss(image_patch_features, text_features)\n",
    "                val_loss += loss.item()\n",
    "    \n",
    "        # Average losses\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Log epoch losses to MLflow\n",
    "        mlflow.log_metric(\"Loss/Epoch_Train\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"Loss/Epoch_Val\", val_loss, step=epoch)\n",
    "    \n",
    "        # Save model checkpoint to MLflow\n",
    "        checkpoint_path = f\"model_epoch_{epoch}.pt\"\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        mlflow.pytorch.log_model(model, artifact_path=f\"model_epoch_{epoch}\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
