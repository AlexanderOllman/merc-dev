{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference on First Image in Dataset\n",
    "\n",
    "This file contains code to load the converted BLIP2 model from MLflow and perform inference on the first image in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import torch\n",
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to S3 to access the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure S3 connection\n",
    "object_storage_service_name = \"source-images-service\"\n",
    "object_storage_namespace = \".ezdata-system\"\n",
    "resource_type = \".svc\"\n",
    "domain = \".cluster.local\"\n",
    "object_storage_port = \"30000\"\n",
    "\n",
    "s3_endpoint_url = f\"http://{object_storage_service_name}{object_storage_namespace}{resource_type}{domain}:{object_storage_port}\"\n",
    "print(f\"S3 endpoint URL: {s3_endpoint_url}\")\n",
    "\n",
    "# Create S3 clients\n",
    "s3_client = boto3.client('s3', endpoint_url=s3_endpoint_url)\n",
    "s3_resource = boto3.resource('s3', endpoint_url=s3_endpoint_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set bucket name\n",
    "bucket_name = \"poc-mercedes-gp\"\n",
    "\n",
    "# Load the dataset JSON file\n",
    "file_key = \"training/training_dataset.json\"\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "content = response[\"Body\"].read().decode(\"utf-8\")\n",
    "dataset = json.loads(content)\n",
    "\n",
    "# Display information about the first image in the dataset\n",
    "print(\"First image information:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download and display the first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the S3 key for the first image\n",
    "first_image_key = dataset[0]['s3_key']\n",
    "print(f\"First image S3 key: {first_image_key}\")\n",
    "\n",
    "# Download the image\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=first_image_key)\n",
    "image_data = response['Body'].read()\n",
    "\n",
    "# Convert to PIL Image\n",
    "image = Image.open(BytesIO(image_data))\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Image: {first_image_key.split('/')[-1]}\\nGround Truth: {dataset[0]['text']}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the converted model from MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh token if needed\n",
    "%update_token\n",
    "\n",
    "# Get the latest version of the registered model\n",
    "client = MlflowClient()\n",
    "registered_model_name = \"blip_ft_production\"\n",
    "\n",
    "# Get the latest version\n",
    "latest_versions = client.get_latest_versions(registered_model_name)\n",
    "latest_version = None\n",
    "for version in latest_versions:\n",
    "    if version.current_stage == \"Production\":\n",
    "        latest_version = version\n",
    "        break\n",
    "    \n",
    "if latest_version is None and len(latest_versions) > 0:\n",
    "    latest_version = latest_versions[0]\n",
    "\n",
    "if latest_version is None:\n",
    "    raise ValueError(f\"No versions found for model {registered_model_name}\")\n",
    "\n",
    "print(f\"Using model version: {latest_version.version}, stage: {latest_version.current_stage}\")\n",
    "\n",
    "# Get the model's artifact URI\n",
    "model_uri = f\"models:/{registered_model_name}/{latest_version.version}\"\n",
    "print(f\"Model URI: {model_uri}\")\n",
    "\n",
    "# Load the model as a PyTorch model\n",
    "scripted_model = mlflow.pytorch.load_model(model_uri, map_location=device)\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocess the image manually (without LAVIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom preprocessing function based on standard BLIP2 preprocessing\n",
    "def preprocess_image(image, size=224):\n",
    "    # BLIP2 standard preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), \n",
    "                             (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])\n",
    "    \n",
    "    # Convert to RGB if it's not already\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "        \n",
    "    # Apply transformations\n",
    "    tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return tensor\n",
    "\n",
    "# Preprocess the image\n",
    "processed_image = preprocess_image(image).to(device)\n",
    "print(f\"Processed image shape: {processed_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run inference with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "scripted_model.eval()\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    features = scripted_model(processed_image)\n",
    "    \n",
    "print(f\"Output features shape: {features.shape}\")\n",
    "\n",
    "# Displaying the first few features\n",
    "print(\"\\nFirst 10 feature values:\")\n",
    "print(features[0, :10].cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Feature Map\n",
    "\n",
    "Let's visualize a projection of the feature vectors to better understand what the model has extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert features to numpy for PCA\n",
    "features_np = features.cpu().numpy().squeeze()\n",
    "\n",
    "# Apply PCA to reduce dimensionality for visualization\n",
    "pca = PCA(n_components=3)\n",
    "features_pca = pca.fit_transform(features_np)\n",
    "\n",
    "# Create a 3D visualization\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the first 100 points (or fewer if there are fewer)\n",
    "num_points = min(100, features_pca.shape[0])\n",
    "scatter = ax.scatter(features_pca[:num_points, 0], \n",
    "           features_pca[:num_points, 1], \n",
    "           features_pca[:num_points, 2],\n",
    "           c=range(num_points), \n",
    "           cmap='viridis')\n",
    "\n",
    "ax.set_title('PCA of Feature Vectors')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "plt.colorbar(scatter, \n",
    "             label='Feature Index')\n",
    "plt.show()\n",
    "\n",
    "# Print variance explained by each component\n",
    "print(\"Variance explained by principal components:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Try to use the features for inference (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the model exposes a text_decoder or projection layer, we might try to use it\n",
    "# This is a simplified example and might need adjustment based on the actual model structure\n",
    "try:\n",
    "    if hasattr(scripted_model, \"generate_text\") and callable(scripted_model.generate_text):\n",
    "        # Some models might have a text generation function\n",
    "        text = scripted_model.generate_text(features)\n",
    "        print(f\"\\nGenerated text: {text}\")\n",
    "        print(f\"Ground truth: {dataset[0]['text']}\")\n",
    "    else:\n",
    "        print(\"The model doesn't have a direct text generation method.\")\n",
    "        print(\"The features might need to be passed to a separate text generation model.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate text: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Alternative: Try direct image input if feature extraction doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the above doesn't work, the model might expect direct image input for full processing\n",
    "try:\n",
    "    # Some TorchScript models might have different input expectations\n",
    "    with torch.no_grad():\n",
    "        alternative_output = scripted_model(processed_image)\n",
    "    \n",
    "    print(\"\\nAlternative direct input output:\")\n",
    "    if isinstance(alternative_output, tuple):\n",
    "        print(f\"Output is a tuple of length {len(alternative_output)}\")\n",
    "        for i, item in enumerate(alternative_output):\n",
    "            if torch.is_tensor(item):\n",
    "                print(f\"Item {i} shape: {item.shape}\")\n",
    "            else:\n",
    "                print(f\"Item {i} type: {type(item)}\")\n",
    "    elif torch.is_tensor(alternative_output):\n",
    "        print(f\"Output shape: {alternative_output.shape}\")\n",
    "    else:\n",
    "        print(f\"Output type: {type(alternative_output)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Alternative approach failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing and Analyzing the Model Output\n",
    "\n",
    "The output tensor has shape `[1, 257, 1408]`, which suggests:\n",
    "- 1: batch size (single image)\n",
    "- 257: sequence length (likely 256 image patches + 1 class token)\n",
    "- 1408: feature dimension for each token\n",
    "\n",
    "Let's visualize this output in various ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we're using the output from the alternative approach\n",
    "features_tensor = alternative_output\n",
    "\n",
    "# Move to CPU and get rid of batch dimension\n",
    "features_cpu = features_tensor.squeeze(0).cpu()  # Shape: [257, 1408]\n",
    "\n",
    "# 1. First, let's look at basic statistics of the features\n",
    "print(f\"Features shape: {features_cpu.shape}\")\n",
    "print(f\"Mean: {features_cpu.mean().item():.4f}\")\n",
    "print(f\"Std: {features_cpu.std().item():.4f}\")\n",
    "print(f\"Min: {features_cpu.min().item():.4f}\")\n",
    "print(f\"Max: {features_cpu.max().item():.4f}\")\n",
    "\n",
    "# 2. Visualize the distribution of feature values\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Flatten all features and take a sample if it's too large\n",
    "all_features = features_cpu.flatten().numpy()\n",
    "if len(all_features) > 10000:\n",
    "    indices = np.random.choice(len(all_features), 10000, replace=False)\n",
    "    all_features = all_features[indices]\n",
    "plt.hist(all_features, bins=50)\n",
    "plt.title('Distribution of Feature Values')\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Visualize the attention pattern across image patches\n",
    "# Extract token importance (using L2 norm as a proxy for importance)\n",
    "token_importance = torch.norm(features_cpu, dim=1).numpy()\n",
    "token_importance = token_importance[1:]  # Remove the class token (first position)\n",
    "\n",
    "# Create a spatial map (assuming 16x16 patches for a 224x224 image)\n",
    "patch_size = 14  # Standard for ViT/BLIP2\n",
    "num_patches_side = int(np.sqrt(len(token_importance)))\n",
    "attention_map = token_importance.reshape(num_patches_side, num_patches_side)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(attention_map, cmap='viridis')\n",
    "plt.title('Token Importance Map')\n",
    "plt.colorbar(label='Feature Magnitude')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualize a 2D projection of the token embeddings (using PCA)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA to reduce dimensionality of patch features\n",
    "pca = PCA(n_components=2)\n",
    "# Include the class token (first token) in this visualization\n",
    "tokens_pca = pca.fit_transform(features_cpu.numpy())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Plot all tokens except the class token\n",
    "plt.scatter(tokens_pca[1:, 0], tokens_pca[1:, 1], alpha=0.5, label='Patch Tokens')\n",
    "# Highlight the class token specially\n",
    "plt.scatter(tokens_pca[0, 0], tokens_pca[0, 1], color='red', s=100, label='Class Token')\n",
    "plt.title('PCA of Token Embeddings')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Extract and analyze the class token (often used for classification)\n",
    "class_token = features_cpu[0]  # First token is the class token\n",
    "print(f\"Class token shape: {class_token.shape}\")\n",
    "\n",
    "# Visualize the top activated features in the class token\n",
    "top_k = 20\n",
    "values, indices = torch.topk(class_token, k=top_k)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(top_k), values.numpy())\n",
    "plt.title(f'Top {top_k} Activated Features in Class Token')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Activation Value')\n",
    "plt.xticks(range(top_k), indices.numpy())\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Compute average similarity between the class token and patch tokens\n",
    "# This helps understand how much the class token aggregates information from all patches\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class_token_np = class_token.numpy().reshape(1, -1)\n",
    "patch_tokens_np = features_cpu[1:].numpy()\n",
    "similarities = cosine_similarity(class_token_np, patch_tokens_np)[0]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(similarities, bins=30)\n",
    "plt.title('Distribution of Patch Similarities to Class Token')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean similarity to class token: {np.mean(similarities):.4f}\")\n",
    "print(f\"Max similarity to class token: {np.max(similarities):.4f}\")\n",
    "print(f\"Min similarity to class token: {np.min(similarities):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "We have:\n",
    "1. Loaded the first image from the S3 dataset\n",
    "2. Displayed the image with its ground truth caption\n",
    "3. Loaded the converted model from MLflow\n",
    "4. Performed manual preprocessing without using LAVIS\n",
    "5. Run inference with the model\n",
    "6. Visualized the extracted features\n",
    "7. Analyzed the model output in detail with multiple visualizations\n",
    "\n",
    "The features extracted by the BLIP2 model show the relationships between different image regions and capture the image content in a high-dimensional space. The class token serves as an aggregate representation of the entire image, which is typically used for downstream tasks like classification or as input to text generation components."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
