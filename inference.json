{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLIP2 Fine-tuned Model Inference\n",
    "\n",
    "This notebook demonstrates how to load the fine-tuned BLIP2 model from MLflow and perform inference on test images to generate captions using cross-attention between image features and text prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import torch\n",
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import traceback\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "print(f\"Using device: {device} with dtype: {model_dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to S3 to access the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure S3 connection\n",
    "object_storage_service_name = \"source-images-service\"\n",
    "object_storage_namespace = \".ezdata-system\"\n",
    "resource_type = \".svc\"\n",
    "domain = \".cluster.local\"\n",
    "object_storage_port = \"30000\"\n",
    "\n",
    "s3_endpoint_url = f\"http://{object_storage_service_name}{object_storage_namespace}{resource_type}{domain}:{object_storage_port}\"\n",
    "print(f\"S3 endpoint URL: {s3_endpoint_url}\")\n",
    "\n",
    "# Create S3 clients\n",
    "s3_client = boto3.client('s3', endpoint_url=s3_endpoint_url)\n",
    "s3_resource = boto3.resource('s3', endpoint_url=s3_endpoint_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set bucket name\n",
    "bucket_name = \"poc-mercedes-gp\"\n",
    "\n",
    "# Load the dataset JSON file\n",
    "file_key = \"training/training_dataset.json\"\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "content = response[\"Body\"].read().decode(\"utf-8\")\n",
    "dataset = json.loads(content)\n",
    "\n",
    "# Display information about the first image in the dataset\n",
    "print(\"First image information:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download and display the first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the S3 key for the first image\n",
    "first_image_key = dataset[0]['s3_key']\n",
    "print(f\"First image S3 key: {first_image_key}\")\n",
    "\n",
    "# Download the image\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=first_image_key)\n",
    "image_data = response['Body'].read()\n",
    "\n",
    "# Convert to PIL Image\n",
    "image = Image.open(BytesIO(image_data))\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Image: {first_image_key.split('/')[-1]}\\nGround Truth: {dataset[0]['text']}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load both the BLIP2 feature extractor and language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh token if needed\n",
    "%update_token\n",
    "\n",
    "# Get the latest version of the registered BLIP2 model\n",
    "client = MlflowClient()\n",
    "registered_model_name = \"blip_ft_production\"\n",
    "\n",
    "# Get the latest version\n",
    "latest_versions = client.get_latest_versions(registered_model_name)\n",
    "latest_version = None\n",
    "\n",
    "for version in latest_versions:\n",
    "    if version.current_stage == \"Production\":\n",
    "        latest_version = version\n",
    "        break\n",
    "    \n",
    "if latest_version is None and len(latest_versions) > 0:\n",
    "    latest_version = latest_versions[0]\n",
    "\n",
    "if latest_version is None:\n",
    "    raise ValueError(f\"No versions found for model {registered_model_name}\")\n",
    "\n",
    "print(f\"Using BLIP2 model version: {latest_version.version}, stage: {latest_version.current_stage}\")\n",
    "\n",
    "# Get the model's artifact URI\n",
    "model_uri = f\"models:/{registered_model_name}/{latest_version.version}\"\n",
    "print(f\"BLIP2 model URI: {model_uri}\")\n",
    "\n",
    "# Load the feature extractor model from MLflow\n",
    "feature_extractor = mlflow.pytorch.load_model(model_uri, map_location=device)\n",
    "feature_extractor.eval()\n",
    "print(\"BLIP2 feature extractor loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the OPT language model\n",
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "import torch.nn as nn\n",
    "\n",
    "# We'll use a smaller OPT model to keep memory usage reasonable\n",
    "model_id = \"facebook/opt-1.3b\"\n",
    "print(f\"Loading language model: {model_id}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "language_model = OPTForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=model_dtype,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "language_model.eval()\n",
    "\n",
    "print(f\"Language model loaded with {sum(p.numel() for p in language_model.parameters())/1e6:.1f}M parameters\")\n",
    "print(f\"Language model dtype: {next(language_model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocess the image and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom preprocessing function based on standard BLIP2 preprocessing\n",
    "def preprocess_image(image, size=224):\n",
    "    # BLIP2 standard preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), \n",
    "                             (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])\n",
    "    \n",
    "    # Convert to RGB if it's not already\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "        \n",
    "    # Apply transformations\n",
    "    tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return tensor\n",
    "\n",
    "# Preprocess the image\n",
    "processed_image = preprocess_image(image).to(device).to(model_dtype)\n",
    "print(f\"Processed image shape: {processed_image.shape}\")\n",
    "\n",
    "# Extract features with BLIP2\n",
    "with torch.no_grad():\n",
    "    image_features = feature_extractor(processed_image)\n",
    "    \n",
    "print(f\"Extracted features shape: {image_features.shape}\")\n",
    "\n",
    "# We'll use the class token (first token) for generation\n",
    "class_token = image_features[:, 0, :].to(device).to(model_dtype)\n",
    "print(f\"Class token shape: {class_token.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define the cross-attention generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for sampling\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering \"\"\"\n",
    "    # Handle both single and batch dimensions\n",
    "    if logits.dim() > 1:\n",
    "        # Apply filtering to each item in the batch\n",
    "        filtered_logits = logits.clone()\n",
    "        for batch_idx in range(logits.size(0)):\n",
    "            filtered_logits[batch_idx] = top_k_top_p_filtering(\n",
    "                logits[batch_idx], top_k=top_k, top_p=top_p, filter_value=filter_value\n",
    "            )\n",
    "        return filtered_logits\n",
    "    \n",
    "    assert logits.dim() == 1  # Handle scalar inputs\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    \n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    \n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        \n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        \n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    \n",
    "    return logits\n",
    "\n",
    "def generate_with_cross_attention(image_features, prompt, max_length=30, existing_projection_layer=None, temperature=0.7):\n",
    "    # Tokenize prompt\n",
    "    encoded_prompt = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = encoded_prompt.input_ids\n",
    "    \n",
    "    # Get model's hidden dimension\n",
    "    hidden_size = language_model.config.hidden_size\n",
    "    print(f\"Language model hidden size: {hidden_size}\")\n",
    "    print(f\"Image features shape: {image_features.shape}\")\n",
    "    \n",
    "    # Use the provided projection layer or create a new one\n",
    "    if existing_projection_layer is not None and isinstance(existing_projection_layer, torch.nn.Module):\n",
    "        if existing_projection_layer.in_features == image_features.shape[-1] and existing_projection_layer.out_features == hidden_size:\n",
    "            projection_layer = existing_projection_layer\n",
    "            print(\"Using existing projection layer\")\n",
    "        else:\n",
    "            print(f\"Existing projection layer dimensions don't match: {existing_projection_layer.in_features} -> {existing_projection_layer.out_features}\")\n",
    "            print(\"Creating new projection layer\")\n",
    "            projection_layer = torch.nn.Linear(image_features.shape[-1], hidden_size).to(device).to(model_dtype)\n",
    "            torch.nn.init.normal_(projection_layer.weight, std=0.01)\n",
    "            torch.nn.init.zeros_(projection_layer.bias)\n",
    "    else:\n",
    "        print(\"Creating new projection layer\")\n",
    "        projection_layer = torch.nn.Linear(image_features.shape[-1], hidden_size).to(device).to(model_dtype)\n",
    "        torch.nn.init.normal_(projection_layer.weight, std=0.01)\n",
    "        torch.nn.init.zeros_(projection_layer.bias)\n",
    "    \n",
    "    # Project image features to match the model's hidden dimension\n",
    "    print(f\"Image features dtype before projection: {image_features.dtype}\")\n",
    "    print(f\"Projection layer weight dtype: {projection_layer.weight.dtype}\")\n",
    "    \n",
    "    projected_img_features = projection_layer(image_features)\n",
    "    print(f\"Projected features shape: {projected_img_features.shape}\")\n",
    "    \n",
    "    # Initialize generation\n",
    "    cur_len = input_ids.shape[1]\n",
    "    context = input_ids.clone()\n",
    "    \n",
    "    # Generate one token at a time\n",
    "    while cur_len < max_length:\n",
    "        with torch.no_grad():\n",
    "            # Get model embeddings\n",
    "            inputs_embeds = language_model.model.decoder.embed_tokens(context)\n",
    "            \n",
    "            # Forward pass through model layers\n",
    "            hidden_states = inputs_embeds\n",
    "            \n",
    "            # Create attention mask and causal mask\n",
    "            attention_mask = torch.ones(context.shape, device=context.device, dtype=torch.long)\n",
    "            seq_length = context.shape[1]\n",
    "            \n",
    "            # Create causal mask - shape must be [batch_size, 1, seq_length, seq_length]\n",
    "            causal_mask = torch.tril(torch.ones((seq_length, seq_length), device=context.device))\n",
    "            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, seq_len]\n",
    "            \n",
    "            # Manually apply each transformer layer with our image features\n",
    "            for i, layer in enumerate(language_model.model.decoder.layers):\n",
    "                # Apply self-attention with causal mask - without use_cache parameter\n",
    "                layer_outputs = layer.self_attn(\n",
    "                    hidden_states=hidden_states,\n",
    "                    attention_mask=causal_mask,\n",
    "                    past_key_value=None\n",
    "                )\n",
    "                hidden_states = layer.self_attn_layer_norm(hidden_states + layer_outputs[0])\n",
    "                \n",
    "                # Simulate cross-attention by adding image features\n",
    "                # Expand image features to match the sequence length\n",
    "                image_features_expanded = projected_img_features.expand(-1, hidden_states.shape[1], -1)\n",
    "                \n",
    "                # Only apply image features to some layers for a balance\n",
    "                if i % 2 == 0:  # Apply to every other layer\n",
    "                    hidden_states = hidden_states + 0.1 * image_features_expanded  # Scale down the influence\n",
    "                \n",
    "                # Apply feed-forward network\n",
    "                residual = hidden_states\n",
    "                hidden_states = layer.final_layer_norm(hidden_states)\n",
    "                hidden_states = layer.fc1(hidden_states)\n",
    "                hidden_states = torch.nn.functional.relu(hidden_states)\n",
    "                hidden_states = layer.fc2(hidden_states)\n",
    "                hidden_states = residual + hidden_states\n",
    "            \n",
    "            # Forward through final layers\n",
    "            hidden_states = language_model.model.decoder.final_layer_norm(hidden_states)\n",
    "            next_token_logits = language_model.lm_head(hidden_states[:, -1, :])\n",
    "            \n",
    "            # Sample next token (with temperature)\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=0.9)\n",
    "            probs = torch.nn.functional.softmax(filtered_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            context = torch.cat([context, next_token], dim=1)\n",
    "            cur_len += 1\n",
    "            \n",
    "            # Stop if we generate an EOS token\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(context[0], skip_special_tokens=True)\n",
    "    return generated_text, projection_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate captions using cross-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug shapes and dtypes\n",
    "print(f\"Class token shape: {class_token.shape}\")\n",
    "print(f\"Language model hidden size: {language_model.config.hidden_size}\")\n",
    "print(f\"Model dtype: {next(language_model.parameters()).dtype}\")\n",
    "\n",
    "# Check if we already have a projection layer\n",
    "try:\n",
    "    print(f\"Projection layer exists: in_features={projection_layer.in_features}, out_features={projection_layer.out_features}\")\n",
    "except NameError:\n",
    "    print(\"No projection layer exists yet, one will be created\")\n",
    "    projection_layer = None\n",
    "\n",
    "# Execute the cross-attention generation\n",
    "try:\n",
    "    print(\"Using cross-attention approach:\")\n",
    "    result, projection_layer = generate_with_cross_attention(\n",
    "        class_token, \n",
    "        \"This image shows\", \n",
    "        existing_projection_layer=projection_layer,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    print(f\"\\nGenerated caption: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with cross-attention approach: {e}\")\n",
    "    traceback.print_exc()\n",
    "    print(\"This approach requires a more detailed implementation with the specific model architecture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Try multiple prompts for diverse captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth caption\n",
    "print(f\"Ground Truth: {dataset[0]['text']}\")\n",
    "print(\"\\nGenerated Captions with Different Prompts:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Try different prompts\n",
    "prompts = [\n",
    "    \"This image shows\",\n",
    "    \"I can see\",\n",
    "    \"The image contains\",\n",
    "    \"This is a picture of\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    try:\n",
    "        result, projection_layer = generate_with_cross_attention(\n",
    "            class_token, \n",
    "            prompt,\n",
    "            existing_projection_layer=projection_layer,\n",
    "            temperature=0.8  # Slightly higher temperature for diversity\n",
    "        )\n",
    "        print(f\"Prompt: '{prompt}'\")\n",
    "        print(f\"Generated: '{result}'\")\n",
    "        print(\"-\" * 60)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating with prompt '{prompt}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Process additional test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process some additional test images\n",
    "test_indices = [1, 2, 3]  # We'll try a few more images from the dataset\n",
    "\n",
    "for idx in test_indices:\n",
    "    if idx < len(dataset):\n",
    "        print(f\"\\nProcessing test image {idx}:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Get image\n",
    "        image_key = dataset[idx]['s3_key']\n",
    "        print(f\"Image S3 key: {image_key}\")\n",
    "        \n",
    "        try:\n",
    "            # Download and process image\n",
    "            response = s3_client.get_object(Bucket=bucket_name, Key=image_key)\n",
    "            image_data = response['Body'].read()\n",
    "            test_image = Image.open(BytesIO(image_data))\n",
    "            \n",
    "            # Display image\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.imshow(test_image)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"Ground Truth: {dataset[idx]['text']}\")\n",
    "            plt.show()\n",
    "            \n",
    "            # Preprocess and extract features\n",
    "            processed_test_image = preprocess_image(test_image).to(device).to(model_dtype)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                test_features = feature_extractor(processed_test_image)\n",
    "                test_class_token = test_features[:, 0, :].to(device).to(model_dtype)\n",
    "            \n",
    "            # Generate caption with the best prompt\n",
    "            result, projection_layer = generate_with_cross_attention(\n",
    "                test_class_token,\n",
    "                \"This image shows\",\n",
    "                existing_projection_layer=projection_layer\n",
    "            )\n",
    "            \n",
    "            print(f\"Ground Truth: {dataset[idx]['text']}\")\n",
    "            print(f\"Generated Caption: {result}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing test image {idx}: {e}\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate caption quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can implement evaluation metrics for captions if needed\n",
    "# Examples: BLEU, ROUGE, etc.\n",
    "# For simplicity, we'll just compare with ground truth manually\n",
    "\n",
    "print(\"Caption Evaluation:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"This implementation uses a cross-attention mechanism to combine image features\")\n",
    "print(\"extracted by the fine-tuned BLIP2 model with a language model for caption generation.\")\n",
    "print(\"\\nThe quality of the generated captions depends on several factors:\")\n",
    "print(\"1. The quality of the BLIP2 fine-tuning on the domain-specific dataset\")\n",
    "print(\"2. The effectiveness of the projection layer in mapping between feature spaces\")\n",
    "print(\"3. The ability of the OPT language model to generate coherent text\")\n",
    "print(\"4. The cross-attention implementation's ability to condition on image features\")\n",
    "print(\"\\nFuture improvements could include:\")\n",
    "print(\"- Fine-tuning the projection layer on image-text pairs\")\n",
    "print(\"- Using a larger language model for better text generation\")\n",
    "print(\"- Implementing true cross-attention rather than feature injection\")\n",
    "print(\"- Adding beam search or other advanced decoding strategies\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
