{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLIP2 Fine-tuned Model Inference\n",
    "\n",
    "This notebook demonstrates how to load the fine-tuned BLIP2 model from MLflow and perform inference on test images to generate captions using cross-attention between image features and text prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import torch\n",
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import traceback\n",
    "import pickle\n",
    "import cloudpickle\n",
    "import bentoml\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "print(f\"Using device: {device} with dtype: {model_dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to S3 to access the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure S3 connection\n",
    "object_storage_service_name = \"source-images-service\"\n",
    "object_storage_namespace = \".ezdata-system\"\n",
    "resource_type = \".svc\"\n",
    "domain = \".cluster.local\"\n",
    "object_storage_port = \"30000\"\n",
    "\n",
    "s3_endpoint_url = f\"http://{object_storage_service_name}{object_storage_namespace}{resource_type}{domain}:{object_storage_port}\"\n",
    "print(f\"S3 endpoint URL: {s3_endpoint_url}\")\n",
    "\n",
    "# Create S3 clients\n",
    "s3_client = boto3.client('s3', endpoint_url=s3_endpoint_url)\n",
    "s3_resource = boto3.resource('s3', endpoint_url=s3_endpoint_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set bucket name\n",
    "bucket_name = \"poc-mercedes-gp\"\n",
    "\n",
    "# Load the dataset JSON file\n",
    "file_key = \"training/training_dataset.json\"\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "content = response[\"Body\"].read().decode(\"utf-8\")\n",
    "dataset = json.loads(content)\n",
    "\n",
    "# Display information about the first image in the dataset\n",
    "print(\"First image information:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download and display the first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the S3 key for the first image\n",
    "first_image_key = dataset[0]['s3_key']\n",
    "print(f\"First image S3 key: {first_image_key}\")\n",
    "\n",
    "# Download the image\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=first_image_key)\n",
    "image_data = response['Body'].read()\n",
    "\n",
    "# Convert to PIL Image\n",
    "image = Image.open(BytesIO(image_data))\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Image: {first_image_key.split('/')[-1]}\\nGround Truth: {dataset[0]['text']}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load both the fine-tuned BLIP model and base BLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh token if needed\n",
    "%update_token\n",
    "\n",
    "# Get the latest version of the registered BLIP2 model\n",
    "client = MlflowClient()\n",
    "registered_model_name = \"blip_ft_production\"\n",
    "\n",
    "# Get the latest version\n",
    "latest_versions = client.get_latest_versions(registered_model_name)\n",
    "latest_version = None\n",
    "\n",
    "for version in latest_versions:\n",
    "    if version.current_stage == \"Production\":\n",
    "        latest_version = version\n",
    "        break\n",
    "    \n",
    "if latest_version is None and len(latest_versions) > 0:\n",
    "    latest_version = latest_versions[0]\n",
    "\n",
    "if latest_version is None:\n",
    "    raise ValueError(f\"No versions found for model {registered_model_name}\")\n",
    "\n",
    "print(f\"Using BLIP2 model version: {latest_version.version}, stage: {latest_version.current_stage}\")\n",
    "\n",
    "# Get the model's artifact URI\n",
    "model_uri = f\"models:/{registered_model_name}/{latest_version.version}\"\n",
    "print(f\"BLIP2 model URI: {model_uri}\")\n",
    "\n",
    "# Load the fine-tuned model from MLflow\n",
    "ft_model = mlflow.pytorch.load_model(model_uri, map_location=device)\n",
    "ft_model = ft_model.to(model_dtype)\n",
    "ft_model.eval()\n",
    "print(f\"Fine-tuned BLIP model loaded successfully with dtype: {next(ft_model.parameters()).dtype}\")\n",
    "\n",
    "# Also load the base BLIP model to help with caption generation\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "base_model_name = \"Salesforce/blip-image-captioning-base\"  # Using the base BLIP model\n",
    "print(f\"Loading base BLIP model: {base_model_name}\")\n",
    "\n",
    "base_processor = BlipProcessor.from_pretrained(base_model_name)\n",
    "base_model = BlipForConditionalGeneration.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=model_dtype\n",
    ").to(device)\n",
    "base_model.eval()\n",
    "print(f\"Base BLIP model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define image preprocessing and caption generation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def preprocess_image(image, size=224):\n",
    "    # Standard BLIP preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), \n",
    "                             (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])\n",
    "    \n",
    "    # Convert to RGB if it's not already\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "        \n",
    "    # Apply transformations\n",
    "    tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return tensor\n",
    "\n",
    "# Function to generate caption with the fine-tuned model\n",
    "# This approach works with TorchScript models that don't have generate() method\n",
    "def generate_caption(image, prompt=\"This image shows\", temperature=0.7):\n",
    "    try:\n",
    "        # Process the image using standard preprocessing\n",
    "        processed_image = preprocess_image(image).to(device, model_dtype)\n",
    "        
    "        # Important: TorchScript models don't have .generate() method
    "        # First, get the image features using the fine-tuned model's forward pass
    "        with torch.no_grad():
    "            # This only extracts features, doesn't generate text
    "            image_features = ft_model(processed_image)  # Get features from fine-tuned model
    "            
    "            # Use the base model for text generation with the processed image
    "            # We're not trying to transfer features between models (which is complex)
    "            # Instead, we're using base_model's generation pipeline with our processed image
    "            inputs = base_processor(image, prompt, return_tensors=\"pt\").to(device, model_dtype)
    "            
    "            # Generate using base model
    "            generated_ids = base_model.generate(
    "                **inputs,
    "                max_length=50,
    "                num_beams=5,
    "                temperature=temperature
    "            )
    "            caption = base_processor.decode(generated_ids[0], skip_special_tokens=True)
    "        
    "        return caption
    "    except Exception as e:
    "        print(f\"Error generating caption: {e}\")
    "        traceback.print_exc()
    "        
    "        # Fallback to using just the base model
    "        try:
    "            print(\"Falling back to using just the base model...\")
    "            inputs = base_processor(image, prompt, return_tensors=\"pt\").to(device, model_dtype)
    "            with torch.no_grad():
    "                generated_ids = base_model.generate(
    "                    **inputs,
    "                    max_length=50,
    "                    num_beams=5,
    "                    temperature=temperature
    "                )
    "                caption = base_processor.decode(generated_ids[0], skip_special_tokens=True)
    "            return f\"[Fallback] {caption}\"
    "        except Exception as e2:
    "            print(f\"Fallback also failed: {e2}\")
    "            return f\"Error generating caption: {str(e)}\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save the model to BentoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bentoml_model_name = \"blip_bento\"\n",
    "bentoml_model_version = \"v1\"\n",
    "\n",
    "# Get model metadata from MLflow\n",
    "run = mlflow.tracking.MlflowClient().get_run(latest_version.run_id)\n",
    "mlflow_metadata = run.data.params if run.data.params else {}\n",
    "\n",
    "# Add additional metadata\n",
    "metadata = {\n",
    "    \"framework\": \"pytorch\",\n",
    "    \"model_type\": \"BLIP2\",\n",
    "    \"mlflow_model_name\": registered_model_name,\n",
    "    \"mlflow_model_version\": latest_version.version,\n",
    "    **mlflow_metadata\n",
    "}\n",
    "\n",
    "# Save the model manually to a file first, then use BentoML's generic Python API\n",
    "model_file = \"blip_model.pkl\"\n",
    "with open(model_file, \"wb\") as f:\n",
    "    cloudpickle.dump(blip_wrapper, f)\n",
    "\n",
    "# Save using BentoML's Python API instead of PyTorch API\n",
    "bentoml_model = bentoml.picklable_model.save_model(\n",
    "    name=bentoml_model_name,\n",
    "    path=model_file,\n",
    "    signatures={\n",
    "        \"__call__\": {\n",
    "            \"batchable\": True,\n",
    "        },\n",
    "    },\n",
    "    metadata=metadata,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate caption for the first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate caption for the first image\n",
    "result = generate_caption(image, prompt=\"This image shows\")\n",
    "print(f\"\\nGround Truth: {dataset[0]['text']}\")\n",
    "print(f\"Generated caption: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Try multiple prompts for diverse captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different prompts\n",
    "print(\"\\nGenerated Captions with Different Prompts:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "prompts = [\n",
    "    \"This image shows\",\n",
    "    \"I can see\",\n",
    "    \"The image contains\",\n",
    "    \"This is a picture of\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    try:\n",
    "        result = generate_caption(image, prompt=prompt, temperature=0.7)\n",
    "        print(f\"Prompt: '{prompt}'\")\n",
    "        print(f\"Generated: '{result}'\")\n",
    "        print(\"-\" * 60)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating with prompt '{prompt}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Process additional test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process some additional test images\n",
    "test_indices = [1, 2, 3]  # We'll try a few more images from the dataset\n",
    "\n",
    "for idx in test_indices:\n",
    "    if idx < len(dataset):\n",
    "        print(f\"\\nProcessing test image {idx}:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Get image\n",
    "        image_key = dataset[idx]['s3_key']\n",
    "        print(f\"Image S3 key: {image_key}\")\n",
    "        \n",
    "        try:\n",
    "            # Download and process image\n",
    "            response = s3_client.get_object(Bucket=bucket_name, Key=image_key)\n",
    "            image_data = response['Body'].read()\n",
    "            test_image = Image.open(BytesIO(image_data))\n",
    "            \n",
    "            # Display image\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.imshow(test_image)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"Ground Truth: {dataset[idx]['text']}\")\n",
    "            plt.show()\n",
    "            \n",
    "            # Generate caption\n",
    "            result = generate_caption(test_image, prompt=\"This image shows\")\n",
    "            \n",
    "            print(f\"Ground Truth: {dataset[idx]['text']}\")\n",
    "            print(f\"Generated Caption: {result}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing test image {idx}: {e}\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate caption quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caption evaluation section\n",
    "print(\"Caption Evaluation:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"This implementation uses the fine-tuned BLIP model directly for caption generation,\")\n",
    "print(\"rather than using it as a feature extractor with a separate language model.\")\n",
    "print(\"\\nThe quality of the generated captions depends on several factors:\")\n",
    "print(\"1. The quality of the BLIP fine-tuning on the domain-specific dataset\")\n",
    "print(\"2. The appropriateness of the prompts used to condition generation\")\n",
    "print(\"3. The generation parameters (temperature, number of beams, etc.)\")\n",
    "print(\"\\nFuture improvements could include:\")\n",
    "print(\"- Using more domain-specific prompts\")\n",
    "print(\"- Experimenting with different generation parameters\")\n",
    "print(\"- Implementing quantitative evaluation metrics (BLEU, ROUGE, etc.)\")\n",
    "print(\"- Gathering human feedback on caption quality\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
