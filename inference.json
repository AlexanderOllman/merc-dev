{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLIP2 Fine-tuned Model Inference\n",
    "\n",
    "This notebook demonstrates how to load the fine-tuned BLIP2 model from MLflow and perform inference on test images to generate captions using cross-attention between image features and text prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import torch\n",
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import traceback\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "print(f\"Using device: {device} with dtype: {model_dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to S3 to access the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure S3 connection\n",
    "object_storage_service_name = \"source-images-service\"\n",
    "object_storage_namespace = \".ezdata-system\"\n",
    "resource_type = \".svc\"\n",
    "domain = \".cluster.local\"\n",
    "object_storage_port = \"30000\"\n",
    "\n",
    "s3_endpoint_url = f\"http://{object_storage_service_name}{object_storage_namespace}{resource_type}{domain}:{object_storage_port}\"\n",
    "print(f\"S3 endpoint URL: {s3_endpoint_url}\")\n",
    "\n",
    "# Create S3 clients\n",
    "s3_client = boto3.client('s3', endpoint_url=s3_endpoint_url)\n",
    "s3_resource = boto3.resource('s3', endpoint_url=s3_endpoint_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set bucket name\n",
    "bucket_name = \"poc-mercedes-gp\"\n",
    "\n",
    "# Load the dataset JSON file\n",
    "file_key = \"training/training_dataset.json\"\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "content = response[\"Body\"].read().decode(\"utf-8\")\n",
    "dataset = json.loads(content)\n",
    "\n",
    "# Display information about the first image in the dataset\n",
    "print(\"First image information:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download and display the first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the S3 key for the first image\n",
    "first_image_key = dataset[0]['s3_key']\n",
    "print(f\"First image S3 key: {first_image_key}\")\n",
    "\n",
    "# Download the image\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=first_image_key)\n",
    "image_data = response['Body'].read()\n",
    "\n",
    "# Convert to PIL Image\n",
    "image = Image.open(BytesIO(image_data))\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Image: {first_image_key.split('/')[-1]}\\nGround Truth: {dataset[0]['text']}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load both the BLIP2 feature extractor and language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh token if needed\n",
    "%update_token\n",
    "\n",
    "# Get the latest version of the registered BLIP2 model\n",
    "client = MlflowClient()\n",
    "registered_model_name = \"blip_ft_production\"\n",
    "\n",
    "# Get the latest version\n",
    "latest_versions = client.get_latest_versions(registered_model_name)\n",
    "latest_version = None\n",
    "\n",
    "for version in latest_versions:\n",
    "    if version.current_stage == \"Production\":\n",
    "        latest_version = version\n",
    "        break\n",
    "    \n",
    "if latest_version is None and len(latest_versions) > 0:\n",
    "    latest_version = latest_versions[0]\n",
    "\n",
    "if latest_version is None:\n",
    "    raise ValueError(f\"No versions found for model {registered_model_name}\")\n",
    "\n",
    "print(f\"Using BLIP2 model version: {latest_version.version}, stage: {latest_version.current_stage}\")\n",
    "\n",
    "# Get the model's artifact URI\n",
    "model_uri = f\"models:/{registered_model_name}/{latest_version.version}\"\n",
    "print(f\"BLIP2 model URI: {model_uri}\")\n",
    "\n",
    "# Load the feature extractor model from MLflow\n",
    "feature_extractor = mlflow.pytorch.load_model(model_uri, map_location=device)\n",
    "feature_extractor = feature_extractor.to(model_dtype)\n",
    "feature_extractor.eval()\n",
    "print(f\"BLIP2 feature extractor loaded successfully with dtype: {next(feature_extractor.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the OPT language model\n",
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "import torch.nn as nn\n",
    "\n",
    "# We'll use a smaller OPT model to keep memory usage reasonable\n",
    "model_id = \"facebook/opt-1.3b\"\n",
    "print(f\"Loading language model: {model_id}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "language_model = OPTForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=model_dtype,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "language_model.eval()\n",
    "\n",
    "print(f\"Language model loaded with {sum(p.numel() for p in language_model.parameters())/1e6:.1f}M parameters\")\n",
    "print(f\"Language model dtype: {next(language_model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocess the image and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom preprocessing function based on standard BLIP2 preprocessing\n",
    "def preprocess_image(image, size=224):\n",
    "    # BLIP2 standard preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), \n",
    "                             (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])\n",
    "    \n",
    "    # Convert to RGB if it's not already\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "        \n",
    "    # Apply transformations\n",
    "    tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return tensor\n",
    "\n",
    "# Preprocess the image\n",
    "processed_image = preprocess_image(image).to(device)\n",
    "print(f\"Processed image shape: {processed_image.shape}\")\n",
    "print(f\"Processed image dtype: {processed_image.dtype}\")\n",
    "\n",
    "# Extract features with BLIP2\n",
    "with torch.no_grad():\n",
    "    # Convert image to match model dtype at the last moment\n",
    "    processed_image = processed_image.to(model_dtype)\n",
    "    # Verify the dtypes match\n",
    "    model_dtype_actual = next(feature_extractor.parameters()).dtype\n",
    "    print(f\"Model dtype: {model_dtype_actual}, Input dtype: {processed_image.dtype}\")\n",
    "    \n",
    "    if processed_image.dtype != model_dtype_actual:\n",
    "        print(f\"WARNING: Converting input from {processed_image.dtype} to {model_dtype_actual}\")\n",
    "        processed_image = processed_image.to(model_dtype_actual)\n",
    "    \n",
    "    # Now extract features\n",
    "    image_features = feature_extractor(processed_image)\n",
    "    \n",
    "print(f\"Extracted features shape: {image_features.shape}\")\n",
    "print(f\"Extracted features dtype: {image_features.dtype}\")\n",
    "\n",
    "# We'll use the class token (first token) for generation\n",
    "class_token = image_features[:, 0, :].to(device)\n",
    "print(f\"Class token shape: {class_token.shape}\")\n",
    "print(f\"Class token dtype: {class_token.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define the cross-attention generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for sampling\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering \"\"\"\n",
    "    # Handle both single and batch dimensions\n",
    "    if logits.dim() > 1:\n",
    "        # Apply filtering to each item in the batch\n",
    "        filtered_logits = logits.clone()\n",
    "        for batch_idx in range(logits.size(0)):\n",
    "            filtered_logits[batch_idx] = top_k_top_p_filtering(\n",
    "                logits[batch_idx], top_k=top_k, top_p=top_p, filter_value=filter_value\n",
    "            )\n",
    "        return filtered_logits\n",
    "    \n",
    "    assert logits.dim() == 1  # Handle scalar inputs\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    \n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    \n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        \n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        \n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    \n",
    "    return logits\n",
    "\n",
    "def generate_with_cross_attention(image_features, prompt, max_length=30, existing_projection_layer=None, temperature=0.7):
        # Tokenize prompt
        encoded_prompt = tokenizer(prompt, return_tensors=\"pt\").to(device)
        input_ids = encoded_prompt.input_ids
        
        # Get model's hidden dimension
        hidden_size = language_model.config.hidden_size
        print(f\"Language model hidden size: {hidden_size}\")
        print(f\"Image features shape: {image_features.shape}\")
        
        # Use the provided projection layer or create a new one
        if existing_projection_layer is not None and isinstance(existing_projection_layer, torch.nn.Module):
            if existing_projection_layer.in_features == image_features.shape[-1] and existing_projection_layer.out_features == hidden_size:
                projection_layer = existing_projection_layer
                print(\"Using existing projection layer\")
            else:
                print(f\"Existing projection layer dimensions don't match: {existing_projection_layer.in_features} -> {existing_projection_layer.out_features}\")
                print(\"Creating new projection layer\")
                projection_layer = torch.nn.Linear(image_features.shape[-1], hidden_size).to(device).to(model_dtype)
                torch.nn.init.normal_(projection_layer.weight, std=0.01)
                torch.nn.init.zeros_(projection_layer.bias)
        else:
            print(\"Creating new projection layer\")
            projection_layer = torch.nn.Linear(image_features.shape[-1], hidden_size).to(device).to(model_dtype)
            torch.nn.init.normal_(projection_layer.weight, std=0.01)
            torch.nn.init.zeros_(projection_layer.bias)
        
        # Project image features to match the model's hidden dimension
        print(f\"Image features dtype before projection: {image_features.dtype}\")
        print(f\"Projection layer weight dtype: {projection_layer.weight.dtype}\")
        
        # Ensure image features have the correct dtype
        if image_features.dtype != projection_layer.weight.dtype:
            image_features = image_features.to(projection_layer.weight.dtype)
            print(f\"Converted image features to {image_features.dtype}\")
            
        projected_img_features = projection_layer(image_features)
        print(f\"Projected features shape: {projected_img_features.shape}\")
        
        # Initialize generation
        cur_len = input_ids.shape[1]
        context = input_ids.clone()
        
        # Generate one token at a time using a simplified approach
        while cur_len < max_length:
            with torch.no_grad():
                # Use a simpler approach - get the embeddings directly
                outputs = language_model(
                    input_ids=context
                )
                
                # Get logits from the output
                next_token_logits = outputs.logits[:, -1, :]
                
                # Modify logits based on image features
                # Convert to same dtype if needed
                if projected_img_features.dtype != next_token_logits.dtype:
                    projected_img_features = projected_img_features.to(next_token_logits.dtype)
                
                # Scale and add the image features influence through a simple linear projection to the logits space
                image_logits = torch.matmul(projected_img_features, language_model.lm_head.weight.t())
                
                # Combine model logits with image influence
                combined_logits = next_token_logits + 0.1 * image_logits
                
                # Sample next token (with temperature)
                combined_logits = combined_logits / temperature
                filtered_logits = top_k_top_p_filtering(combined_logits, top_k=50, top_p=0.9)
                probs = torch.nn.functional.softmax(filtered_logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
                
                context = torch.cat([context, next_token], dim=1)
                cur_len += 1
                
                # Stop if we generate an EOS token
                if next_token.item() == tokenizer.eos_token_id:
                    break
        
        # Decode the generated text
        generated_text = tokenizer.decode(context[0], skip_special_tokens=True)
        return generated_text, projection_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate captions using cross-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug shapes and dtypes\n",
    "print(f\"Class token shape: {class_token.shape}\")\n",
    "print(f\"Language model hidden size: {language_model.config.hidden_size}\")\n",
    "print(f\"Model dtype: {next(language_model.parameters()).dtype}\")\n",
    "\n",
    "# Check if we already have a projection layer\n",
    "try:\n",
    "    # First check if the variable exists and is not None\n",
    "    if 'projection_layer' in locals() and projection_layer is not None:\n",
    "        print(f\"Projection layer exists: in_features={projection_layer.in_features}, out_features={projection_layer.out_features}\")\n",
    "    else:\n",
    "        print(\"No projection layer exists yet, one will be created\")\n",
    "        projection_layer = None\n",
    "except Exception as e:\n",
    "    print(f\"Error checking projection layer: {e}\")\n",
    "    print(\"Creating a new projection layer\")\n",
    "    projection_layer = None\n",
    "\n",
    "# Execute the cross-attention generation\n",
    "try:\n",
    "    print(\"Using cross-attention approach:\")\n",
    "    result, projection_layer = generate_with_cross_attention(\n",
    "        class_token, \n",
    "        \"This image shows\", \n",
    "        existing_projection_layer=projection_layer,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    print(f\"\\nGenerated caption: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with cross-attention approach: {e}\")\n",
    "    traceback.print_exc()\n",
    "    print(\"This approach requires a more detailed implementation with the specific model architecture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Try multiple prompts for diverse captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth caption\n",
    "print(f\"Ground Truth: {dataset[0]['text']}\")\n",
    "print(\"\\nGenerated Captions with Different Prompts:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Try different prompts\n",
    "prompts = [\n",
    "    \"This image shows\",\n",
    "    \"I can see\",\n",
    "    \"The image contains\",\n",
    "    \"This is a picture of\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    try:\n",
    "        result, projection_layer = generate_with_cross_attention(\n",
    "            class_token, \n",
    "            prompt,\n",
    "            existing_projection_layer=projection_layer,\n",
    "            temperature=0.8  # Slightly higher temperature for diversity\n",
    "        )\n",
    "        print(f\"Prompt: '{prompt}'\")\n",
    "        print(f\"Generated: '{result}'\")\n",
    "        print(\"-\" * 60)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating with prompt '{prompt}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Process additional test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process some additional test images\n",
    "test_indices = [1, 2, 3]  # We'll try a few more images from the dataset\n",
    "\n",
    "for idx in test_indices:\n",
    "    if idx < len(dataset):\n",
    "        print(f\"\\nProcessing test image {idx}:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Get image\n",
    "        image_key = dataset[idx]['s3_key']\n",
    "        print(f\"Image S3 key: {image_key}\")\n",
    "        \n",
    "        try:\n",
    "            # Download and process image\n",
    "            response = s3_client.get_object(Bucket=bucket_name, Key=image_key)\n",
    "            image_data = response['Body'].read()\n",
    "            test_image = Image.open(BytesIO(image_data))\n",
    "            \n",
    "            # Display image\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.imshow(test_image)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"Ground Truth: {dataset[idx]['text']}\")\n",
    "            plt.show()\n",
    "            \n",
    "            # Preprocess and extract features
            processed_test_image = preprocess_image(test_image).to(device)
            
            with torch.no_grad():
                # Convert test image to match model dtype
                processed_test_image = processed_test_image.to(model_dtype)
                test_features = feature_extractor(processed_test_image)
                test_class_token = test_features[:, 0, :].to(device)
            
            # Generate caption with the best prompt
            result, projection_layer = generate_with_cross_attention(\n",
    "                test_class_token,\n",
    "                \"This image shows\",\n",
    "                existing_projection_layer=projection_layer\n",
    "            )\n",
    "            \n",
    "            print(f\"Ground Truth: {dataset[idx]['text']}\")\n",
    "            print(f\"Generated Caption: {result}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing test image {idx}: {e}\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate caption quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can implement evaluation metrics for captions if needed\n",
    "# Examples: BLEU, ROUGE, etc.\n",
    "# For simplicity, we'll just compare with ground truth manually\n",
    "\n",
    "print(\"Caption Evaluation:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"This implementation uses a cross-attention mechanism to combine image features\")\n",
    "print(\"extracted by the fine-tuned BLIP2 model with a language model for caption generation.\")\n",
    "print(\"\\nThe quality of the generated captions depends on several factors:\")\n",
    "print(\"1. The quality of the BLIP2 fine-tuning on the domain-specific dataset\")\n",
    "print(\"2. The effectiveness of the projection layer in mapping between feature spaces\")\n",
    "print(\"3. The ability of the OPT language model to generate coherent text\")\n",
    "print(\"4. The cross-attention implementation's ability to condition on image features\")\n",
    "print(\"\\nFuture improvements could include:\")\n",
    "print(\"- Fine-tuning the projection layer on image-text pairs\")\n",
    "print(\"- Using a larger language model for better text generation\")\n",
    "print(\"- Implementing true cross-attention rather than feature injection\")\n",
    "print(\"- Adding beam search or other advanced decoding strategies\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
