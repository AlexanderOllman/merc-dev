{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference on First Image in Dataset\n",
    "\n",
    "This file contains code to load the converted BLIP2 model from MLflow and perform inference on the first image in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import torch\n",
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to S3 to access the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure S3 connection\n",
    "object_storage_service_name = \"source-images-service\"\n",
    "object_storage_namespace = \".ezdata-system\"\n",
    "resource_type = \".svc\"\n",
    "domain = \".cluster.local\"\n",
    "object_storage_port = \"30000\"\n",
    "\n",
    "s3_endpoint_url = f\"http://{object_storage_service_name}{object_storage_namespace}{resource_type}{domain}:{object_storage_port}\"\n",
    "print(f\"S3 endpoint URL: {s3_endpoint_url}\")\n",
    "\n",
    "# Create S3 clients\n",
    "s3_client = boto3.client('s3', endpoint_url=s3_endpoint_url)\n",
    "s3_resource = boto3.resource('s3', endpoint_url=s3_endpoint_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set bucket name\n",
    "bucket_name = \"poc-mercedes-gp\"\n",
    "\n",
    "# Load the dataset JSON file\n",
    "file_key = \"training/training_dataset.json\"\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "content = response[\"Body\"].read().decode(\"utf-8\")\n",
    "dataset = json.loads(content)\n",
    "\n",
    "# Display information about the first image in the dataset\n",
    "print(\"First image information:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download and display the first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the S3 key for the first image\n",
    "first_image_key = dataset[0]['s3_key']\n",
    "print(f\"First image S3 key: {first_image_key}\")\n",
    "\n",
    "# Download the image\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=first_image_key)\n",
    "image_data = response['Body'].read()\n",
    "\n",
    "# Convert to PIL Image\n",
    "image = Image.open(BytesIO(image_data))\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Image: {first_image_key.split('/')[-1]}\\nGround Truth: {dataset[0]['text']}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the converted model from MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh token if needed\n",
    "%update_token\n",
    "\n",
    "# Get the latest version of the registered model\n",
    "client = MlflowClient()\n",
    "registered_model_name = \"blip_ft_production\"\n",
    "\n",
    "# Get the latest version\n",
    "latest_versions = client.get_latest_versions(registered_model_name)\n",
    "latest_version = None\n",
    "for version in latest_versions:\n",
    "    if version.current_stage == \"Production\":\n",
    "        latest_version = version\n",
    "        break\n",
    "    \n",
    "if latest_version is None and len(latest_versions) > 0:\n",
    "    latest_version = latest_versions[0]\n",
    "\n",
    "if latest_version is None:\n",
    "    raise ValueError(f\"No versions found for model {registered_model_name}\")\n",
    "\n",
    "print(f\"Using model version: {latest_version.version}, stage: {latest_version.current_stage}\")\n",
    "\n",
    "# Get the model's artifact URI\n",
    "model_uri = f\"models:/{registered_model_name}/{latest_version.version}\"\n",
    "print(f\"Model URI: {model_uri}\")\n",
    "\n",
    "# Load the model as a PyTorch model\n",
    "scripted_model = mlflow.pytorch.load_model(model_uri, map_location=device)\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocess the image manually (without LAVIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom preprocessing function based on standard BLIP2 preprocessing\n",
    "def preprocess_image(image, size=224):\n",
    "    # BLIP2 standard preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), \n",
    "                             (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])\n",
    "    \n",
    "    # Convert to RGB if it's not already\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "        \n",
    "    # Apply transformations\n",
    "    tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return tensor\n",
    "\n",
    "# Preprocess the image\n",
    "processed_image = preprocess_image(image).to(device)\n",
    "print(f\"Processed image shape: {processed_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run inference with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "scripted_model.eval()\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    features = scripted_model(processed_image)\n",
    "    \n",
    "print(f\"Output features shape: {features.shape}\")\n",
    "\n",
    "# Displaying the first few features\n",
    "print(\"\\nFirst 10 feature values:\")\n",
    "print(features[0, :10].cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Feature Map\n",
    "\n",
    "Let's visualize a projection of the feature vectors to better understand what the model has extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert features to numpy for PCA\n",
    "features_np = features.cpu().numpy().squeeze()\n",
    "\n",
    "# Apply PCA to reduce dimensionality for visualization\n",
    "pca = PCA(n_components=3)\n",
    "features_pca = pca.fit_transform(features_np)\n",
    "\n",
    "# Create a 3D visualization\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the first 100 points (or fewer if there are fewer)\n",
    "num_points = min(100, features_pca.shape[0])\n",
    "scatter = ax.scatter(features_pca[:num_points, 0], \n",
    "           features_pca[:num_points, 1], \n",
    "           features_pca[:num_points, 2],\n",
    "           c=range(num_points), \n",
    "           cmap='viridis')\n",
    "\n",
    "ax.set_title('PCA of Feature Vectors')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "plt.colorbar(scatter, \n",
    "             label='Feature Index')\n",
    "plt.show()\n",
    "\n",
    "# Print variance explained by each component\n",
    "print(\"Variance explained by principal components:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Try to use the features for inference (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the model exposes a text_decoder or projection layer, we might try to use it\n",
    "# This is a simplified example and might need adjustment based on the actual model structure\n",
    "try:\n",
    "    if hasattr(scripted_model, \"generate_text\") and callable(scripted_model.generate_text):\n",
    "        # Some models might have a text generation function\n",
    "        text = scripted_model.generate_text(features)\n",
    "        print(f\"\\nGenerated text: {text}\")\n",
    "        print(f\"Ground truth: {dataset[0]['text']}\")\n",
    "    else:\n",
    "        print(\"The model doesn't have a direct text generation method.\")\n",
    "        print(\"The features might need to be passed to a separate text generation model.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate text: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Alternative: Try direct image input if feature extraction doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the above doesn't work, the model might expect direct image input for full processing\n",
    "try:\n",
    "    # Some TorchScript models might have different input expectations\n",
    "    with torch.no_grad():\n",
    "        alternative_output = scripted_model(processed_image)\n",
    "    \n",
    "    print(\"\\nAlternative direct input output:\")\n",
    "    if isinstance(alternative_output, tuple):\n",
    "        print(f\"Output is a tuple of length {len(alternative_output)}\")\n",
    "        for i, item in enumerate(alternative_output):\n",
    "            if torch.is_tensor(item):\n",
    "                print(f\"Item {i} shape: {item.shape}\")\n",
    "            else:\n",
    "                print(f\"Item {i} type: {type(item)}\")\n",
    "    elif torch.is_tensor(alternative_output):\n",
    "        print(f\"Output shape: {alternative_output.shape}\")\n",
    "    else:\n",
    "        print(f\"Output type: {type(alternative_output)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Alternative approach failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing and Analyzing the Model Output\n",
    "\n",
    "The output tensor has shape `[1, 257, 1408]`, which suggests:\n",
    "- 1: batch size (single image)\n",
    "- 257: sequence length (likely 256 image patches + 1 class token)\n",
    "- 1408: feature dimension for each token\n",
    "\n",
    "Let's visualize this output in various ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we're using the output from the alternative approach\n",
    "features_tensor = alternative_output\n",
    "\n",
    "# Move to CPU and get rid of batch dimension\n",
    "features_cpu = features_tensor.squeeze(0).cpu()  # Shape: [257, 1408]\n",
    "\n",
    "# 1. First, let's look at basic statistics of the features\n",
    "print(f\"Features shape: {features_cpu.shape}\")\n",
    "print(f\"Mean: {features_cpu.mean().item():.4f}\")\n",
    "print(f\"Std: {features_cpu.std().item():.4f}\")\n",
    "print(f\"Min: {features_cpu.min().item():.4f}\")\n",
    "print(f\"Max: {features_cpu.max().item():.4f}\")\n",
    "\n",
    "# 2. Visualize the distribution of feature values\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Flatten all features and take a sample if it's too large\n",
    "all_features = features_cpu.flatten().numpy()\n",
    "if len(all_features) > 10000:\n",
    "    indices = np.random.choice(len(all_features), 10000, replace=False)\n",
    "    all_features = all_features[indices]\n",
    "plt.hist(all_features, bins=50)\n",
    "plt.title('Distribution of Feature Values')\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Visualize the attention pattern across image patches\n",
    "# Extract token importance (using L2 norm as a proxy for importance)\n",
    "token_importance = torch.norm(features_cpu, dim=1).numpy()\n",
    "token_importance = token_importance[1:]  # Remove the class token (first position)\n",
    "\n",
    "# Create a spatial map (assuming 16x16 patches for a 224x224 image)\n",
    "patch_size = 14  # Standard for ViT/BLIP2\n",
    "num_patches_side = int(np.sqrt(len(token_importance)))\n",
    "attention_map = token_importance.reshape(num_patches_side, num_patches_side)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(attention_map, cmap='viridis')\n",
    "plt.title('Token Importance Map')\n",
    "plt.colorbar(label='Feature Magnitude')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualize a 2D projection of the token embeddings (using PCA)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA to reduce dimensionality of patch features\n",
    "pca = PCA(n_components=2)\n",
    "# Include the class token (first token) in this visualization\n",
    "tokens_pca = pca.fit_transform(features_cpu.numpy())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Plot all tokens except the class token\n",
    "plt.scatter(tokens_pca[1:, 0], tokens_pca[1:, 1], alpha=0.5, label='Patch Tokens')\n",
    "# Highlight the class token specially\n",
    "plt.scatter(tokens_pca[0, 0], tokens_pca[0, 1], color='red', s=100, label='Class Token')\n",
    "plt.title('PCA of Token Embeddings')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Extract and analyze the class token (often used for classification)\n",
    "class_token = features_cpu[0]  # First token is the class token\n",
    "print(f\"Class token shape: {class_token.shape}\")\n",
    "\n",
    "# Visualize the top activated features in the class token\n",
    "top_k = 20\n",
    "values, indices = torch.topk(class_token, k=top_k)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(top_k), values.numpy())\n",
    "plt.title(f'Top {top_k} Activated Features in Class Token')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Activation Value')\n",
    "plt.xticks(range(top_k), indices.numpy())\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Compute average similarity between the class token and patch tokens\n",
    "# This helps understand how much the class token aggregates information from all patches\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class_token_np = class_token.numpy().reshape(1, -1)\n",
    "patch_tokens_np = features_cpu[1:].numpy()\n",
    "similarities = cosine_similarity(class_token_np, patch_tokens_np)[0]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(similarities, bins=30)\n",
    "plt.title('Distribution of Patch Similarities to Class Token')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean similarity to class token: {np.mean(similarities):.4f}\")\n",
    "print(f\"Max similarity to class token: {np.max(similarities):.4f}\")\n",
    "print(f\"Min similarity to class token: {np.min(similarities):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "We have:\n",
    "1. Loaded the first image from the S3 dataset\n",
    "2. Displayed the image with its ground truth caption\n",
    "3. Loaded the converted model from MLflow\n",
    "4. Performed manual preprocessing without using LAVIS\n",
    "5. Run inference with the model\n",
    "6. Visualized the extracted features\n",
    "7. Analyzed the model output in detail with multiple visualizations\n",
    "\n",
    "The features extracted by the BLIP2 model show the relationships between different image regions and capture the image content in a high-dimensional space. The class token serves as an aggregate representation of the entire image, which is typically used for downstream tasks like classification or as input to text generation components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Implementing Text Generation with OPT\n",
    "\n",
    "Now let's use the features extracted by our BLIP2 model to generate text captions using the OPT language model, which is the language model component used in BLIP2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies if not already installed\n",
    "!pip install transformers accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check if we have a GPU available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load OPT model and tokenizer\n",
    "# We'll use a smaller OPT model to keep memory usage reasonable\n",
    "model_id = \"facebook/opt-1.3b\"\n",
    "print(f\"Loading OPT model: {model_id}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "language_model = OPTForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"Model loaded with {sum(p.numel() for p in language_model.parameters())/1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a projection layer to match dimensions between our features and OPT's embedding space\n",
    "# First, get our feature dimensions\n",
    "features_dim = features_tensor.size(-1)  # Should be 1408 based on earlier output\n",
    "embedding_dim = language_model.config.word_embed_proj_dim\n",
    "\n",
    "print(f\"Features dimension: {features_dim}\")\n",
    "print(f\"Language model embedding dimension: {embedding_dim}\")\n",
    "\n",
    "# Create a simple linear projection layer\n",
    "projection_layer = nn.Linear(features_dim, embedding_dim).to(device)\n",
    "\n",
    "# Initialize with normal distribution\n",
    "with torch.no_grad():\n",
    "    projection_layer.weight.normal_(mean=0.0, std=0.02)\n",
    "    projection_layer.bias.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Project image features to language model's embedding space\n",
    "# We'll use the class token as it aggregates information from all patches\n",
    "class_token = features_tensor[0, 0, :].to(device)  # Shape: [1408]\n",
    "print(f\"Class token shape: {class_token.shape}\")\n",
    "\n",
    "# Project the class token\n",
    "with torch.no_grad():\n",
    "    projected_features = projection_layer(class_token).unsqueeze(0)  # Shape: [1, embedding_dim]\n",
    "    print(f\"Projected features shape: {projected_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Generate text with language model using different prompts\n",
    "prompts = [\n",
    "    \"This image shows\",\n",
    "    \"The picture contains\",\n",
    "    \"In this photo I can see\",\n",
    "    \"The scene depicts\"\n",
    "]\n",
    "\n",
    "generation_params = {\n",
    "    \"max_new_tokens\": 30,\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.7,\n",
    "    \"num_return_sequences\": 1\n",
    "}\n",
    "\n",
    "print(\"Generated captions:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create a custom embedding function\n",
    "def add_projected_embeddings(input_ids):\n",
    "    # Get the original embeddings from the model\n",
    "    inputs_embeds = language_model.model.decoder.embed_tokens(input_ids)\n",
    "    \n",
    "    # Combine with our projected image features\n",
    "    # We'll use a simple approach - concatenate at the beginning\n",
    "    batch_size = input_ids.size(0)\n",
    "    image_embeds = projected_features.repeat(batch_size, 1, 1)  # Repeat for batch size\n",
    "    \n",
    "    # Concatenate image embeddings with text embeddings\n",
    "    combined_embeds = torch.cat([image_embeds, inputs_embeds], dim=1)\n",
    "    \n",
    "    return combined_embeds\n",
    "\n",
    "for prompt in prompts:\n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Simple approach: Use projected features to condition the language model\n",
    "        # by prepending them to the input embeddings\n",
    "        inputs_embeds = add_projected_embeddings(input_ids)\n",
    "        \n",
    "        # We need to adjust the attention mask to account for the added embedding\n",
    "        image_attention = torch.ones(\n",
    "            (input_ids.size(0), 1), \n",
    "            dtype=attention_mask.dtype, \n",
    "            device=device\n",
    "        )\n",
    "        combined_attention_mask = torch.cat([image_attention, attention_mask], dim=1)\n",
    "        \n",
    "        # Generate text\n",
    "        outputs = language_model.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=combined_attention_mask,\n",
    "            **generation_params\n",
    "        )\n",
    "        \n",
    "        # Get only the newly generated tokens\n",
    "        generated_tokens = outputs[:, input_ids.size(1)+1:]\n",
    "        generated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "        \n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"Generated: '{prompt} {generated_text}'\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Alternative approach: Use OPT's encoder-decoder like architecture with cross-attention\n",
    "# This approach is more sophisticated and closer to how BLIP2 originally works\n",
    "def generate_with_cross_attention(image_features, prompt, max_length=30):\n",
    "    # Tokenize prompt\n",
    "    encoded_prompt = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = encoded_prompt.input_ids\n",
    "    \n",
    "    # Project image features\n",
    "    projected_img_features = projection_layer(image_features).to(device)\n",
    "    \n",
    "    # Initialize generation\n",
    "    cur_len = input_ids.shape[1]\n",
    "    context = input_ids.clone()\n",
    "    \n",
    "    # Generate one token at a time\n",
    "    while cur_len < max_length:\n",
    "        with torch.no_grad():\n",
    "            # Get model embeddings\n",
    "            inputs_embeds = language_model.model.decoder.embed_tokens(context)\n",
    "            \n",
    "            # Forward pass through model layers\n",
    "            hidden_states = inputs_embeds\n",
    "            \n",
    "            # Manually apply each transformer layer with our image features\n",
    "            for layer in language_model.model.decoder.layers:\n",
    "                # Apply self-attention with causal mask\n",
    "                causal_mask = language_model.model.decoder._prepare_decoder_attention_mask(\n",
    "                    torch.ones_like(context), \n",
    "                    inputs_embeds.shape, \n",
    "                    context.device, \n",
    "                    None\n",
    "                )\n",
    "                \n",
    "                layer_outputs = layer.self_attn(\n",
    "                    hidden_states=hidden_states,\n",
    "                    attention_mask=causal_mask,\n",
    "                    layer_head_mask=None,\n",
    "                    past_key_value=None,\n",
    "                    output_attentions=False,\n",
    "                    use_cache=False\n",
    "                )\n",
    "                hidden_states = layer.self_attn_layer_norm(hidden_states + layer_outputs[0])\n",
    "                \n",
    "                # Here we can simulate cross-attention between text and image features\n",
    "                # For simplicity, we'll just add the projected image features to each token\n",
    "                # A proper implementation would need a cross-attention mechanism\n",
    "                hidden_states = hidden_states + projected_img_features.unsqueeze(1)\n",
    "                \n",
    "                # Apply feed-forward network\n",
    "                residual = hidden_states\n",
    "                hidden_states = layer.final_layer_norm(hidden_states)\n",
    "                hidden_states = layer.fc1(hidden_states)\n",
    "                hidden_states = torch.nn.functional.relu(hidden_states)\n",
    "                hidden_states = layer.fc2(hidden_states)\n",
    "                hidden_states = residual + hidden_states\n",
    "            \n",
    "            # Forward through final layers\n",
    "            hidden_states = language_model.model.decoder.final_layer_norm(hidden_states)\n",
    "            next_token_logits = language_model.lm_head(hidden_states[:, -1, :])\n",
    "            \n",
    "            # Get next token\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "            context = torch.cat([context, next_token], dim=1)\n",
    "            cur_len += 1\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(context[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# This is quite complex and may not work perfectly - for demonstration purposes\n",
    "# Comment out the execution if you encounter issues\n",
    "try:\n",
    "    print(\"Using cross-attention approach:\")\n",
    "    result = generate_with_cross_attention(class_token, \"This image shows\")\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"Error with cross-attention approach: {e}\")\n",
    "    print(\"This approach requires a more detailed implementation with the specific model architecture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Compare Generated Captions with Ground Truth\n",
    "\n",
    "Let's compare our generated captions with the ground truth caption from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ground truth caption\n",
    "ground_truth = dataset[0]['text']\n",
    "print(f\"Ground Truth: {ground_truth}\")\n",
    "print(\"\\nGenerated Captions:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Simple function to try multiple prompts\n",
    "def generate_caption(prompt):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Use the simple concatenation approach from above\n",
    "        inputs_embeds = add_projected_embeddings(input_ids)\n",
    "        \n",
    "        # Adjust attention mask\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        image_attention = torch.ones((input_ids.size(0), 1), dtype=attention_mask.dtype, device=device)\n",
    "        combined_attention_mask = torch.cat([image_attention, attention_mask], dim=1)\n",
    "        \n",
    "        # Generate with different parameters for diversity\n",
    "        outputs = language_model.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=combined_attention_mask,\n",
    "            max_new_tokens=40,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "        \n",
    "        # Get only the new tokens\n",
    "        generated_tokens = outputs[:, input_ids.size(1)+1:]\n",
    "        generated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "        \n",
    "    return f\"{prompt} {generated_text}\"\n",
    "\n",
    "# Generate with different parameters and prompts\n",
    "for i, prompt in enumerate([\n",
    "    \"This image shows\",\n",
    "    \"I can see\",\n",
    "    \"The image contains\",\n",
    "    \"This is a picture of\"\n",
    "]):\n",
    "    caption = generate_caption(prompt)\n",
    "    print(f\"Caption {i+1}: {caption}\")\n",
    "    \n",
    "print(\"\\nNote: The quality of these captions depends on how well our projection layer maps the image features to the language model's embedding space. Fine-tuning this projection would yield better results.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
