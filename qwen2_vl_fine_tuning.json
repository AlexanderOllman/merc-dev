{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Qwen2-VL-7B with TRL on S3 Image-Text Dataset\n",
    "\n",
    "This notebook demonstrates how to fine-tune the Qwen2-VL-7B Vision Language Model using the Hugging Face Transformer Reinforcement Learning (TRL) library on an S3-hosted image-text pair dataset. The trained model will be able to better understand the relationship between images and their descriptions.\n",
    "\n",
    "**Note:** This notebook requires substantial computational resources, preferably a GPU with at least 24GB VRAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "First, we'll install all necessary libraries for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git datasets bitsandbytes peft qwen-vl-utils wandb accelerate boto3 pandas pillow\n",
    "# Using specific versions that work well together\n",
    "# transformers==4.47.0.dev0, trl==0.12.0.dev0, datasets==3.0.2, bitsandbytes==0.44.1, peft==0.13.2, qwen-vl-utils==0.0.8, wandb==0.18.5, accelerate==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install specific PyTorch version to avoid compatibility issues\n",
    "!pip install -q torch==2.4.1+cu121 torchvision==0.19.1+cu121 torchaudio==2.4.1+cu121 --extra-index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Log in to Hugging Face\n",
    "\n",
    "To save your fine-tuned model, you'll need to authenticate with your Hugging Face account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare S3 Dataset\n",
    "\n",
    "We'll load the image-text pairs from S3, prepare the dataset for training, and format it into a chat template for the Qwen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "from PIL import Image\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import json\n",
    "from io import BytesIO\n",
    "import random\n",
    "\n",
    "# Configure S3 access\n",
    "# For AWS authentication, make sure you have valid credentials configured\n",
    "# Either via environment variables, AWS credentials file, or IAM role\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# S3 bucket and dataset information - adjust these values\n",
    "BUCKET_NAME = \"your-s3-bucket-name\"\n",
    "METADATA_FILE = \"metadata.json\"  # Path to your metadata file in S3\n",
    "IMAGES_PREFIX = \"images/\"  # Prefix for image files in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load metadata from S3\n",
    "def load_metadata_from_s3():\n",
    "    response = s3_client.get_object(Bucket=BUCKET_NAME, Key=METADATA_FILE)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    metadata = json.loads(content)\n",
    "    return metadata\n",
    "\n",
    "# Function to load an image from S3\n",
    "def load_image_from_s3(image_key):\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=BUCKET_NAME, Key=f\"{IMAGES_PREFIX}{image_key}\")\n",
    "        image_content = response['Body'].read()\n",
    "        image = Image.open(BytesIO(image_content))\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_key}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load metadata\n",
    "print(\"Loading metadata from S3...\")\n",
    "try:\n",
    "    metadata = load_metadata_from_s3()\n",
    "    print(f\"Loaded metadata with {len(metadata)} entries\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading metadata: {e}\")\n",
    "    # Example fallback metadata for testing purposes\n",
    "    metadata = [\n",
    "        {\"image_file\": \"example1.jpg\", \"text\": \"A beautiful sunset over the mountains.\"},\n",
    "        {\"image_file\": \"example2.jpg\", \"text\": \"A cat playing with a ball of yarn.\"},\n",
    "        # Add more examples as needed\n",
    "    ]\n",
    "    print(\"Using fallback metadata for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System message for the VLM\n",
    "system_message = \"\"\"You are a Vision Language Model trained to understand and describe images accurately.\n",
    "When presented with an image, provide a detailed and accurate description of what you see.\n",
    "Focus on the key elements, objects, people, actions, and context visible in the image.\"\"\"\n",
    "\n",
    "# Function to format data into chat format\n",
    "def format_data(sample):\n",
    "    image = sample.get(\"image\")\n",
    "    text = sample.get(\"text\")\n",
    "    \n",
    "    if image is None or text is None:\n",
    "        return None\n",
    "    \n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": image,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Describe this image in detail.\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": text}],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "# Create a dataset with images and text\n",
    "print(\"Preparing dataset...\")\n",
    "dataset_entries = []\n",
    "\n",
    "# Process a subset for demonstration purposes\n",
    "# In a real scenario, you might want to process all entries\n",
    "sample_size = min(500, len(metadata))  # Limit to 500 samples or less for testing\n",
    "sample_metadata = random.sample(metadata, sample_size)\n",
    "\n",
    "for i, entry in enumerate(sample_metadata):\n",
    "    if i % 50 == 0:  # Print progress indicator\n",
    "        print(f\"Processing entry {i}/{sample_size}\")\n",
    "        \n",
    "    image = load_image_from_s3(entry['image_file'])\n",
    "    if image is not None:\n",
    "        dataset_entries.append({\n",
    "            \"image\": image,\n",
    "            \"text\": entry['text']\n",
    "        })\n",
    "\n",
    "print(f\"Successfully processed {len(dataset_entries)} entries\")\n",
    "\n",
    "# Split into train and validation sets\n",
    "random.shuffle(dataset_entries)\n",
    "split_idx = int(len(dataset_entries) * 0.9)  # 90% train, 10% validation\n",
    "train_entries = dataset_entries[:split_idx]\n",
    "val_entries = dataset_entries[split_idx:]\n",
    "\n",
    "# Format data for chat-based training\n",
    "train_dataset = [format_data(sample) for sample in train_entries if format_data(sample) is not None]\n",
    "eval_dataset = [format_data(sample) for sample in val_entries if format_data(sample) is not None]\n",
    "\n",
    "print(f\"Final dataset sizes: Train={len(train_dataset)}, Validation={len(eval_dataset)}\")\n",
    "\n",
    "# Look at an example\n",
    "if len(train_dataset) > 0:\n",
    "    print(\"\\nExample input:\")\n",
    "    print(f\"User prompt: {train_dataset[0][1]['content'][1]['text']}\")\n",
    "    print(f\"Expected response: {train_dataset[0][2]['content'][0]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the Base Model and Check Initial Performance\n",
    "\n",
    "Let's load the Qwen2-VL-7B model and check its performance on our dataset before fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "\n",
    "model_id = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "\n",
    "# Load model and processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "processor = Qwen2VLProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to process vision inputs for the model\n",
    "def process_vision_info(example):\n",
    "    vision_input = []\n",
    "    \n",
    "    for message in example[:2]:  # Only process system and user messages\n",
    "        for content in message[\"content\"]:\n",
    "            if isinstance(content, dict) and content.get(\"type\") == \"image\":\n",
    "                vision_input.append(content.get(\"image\"))\n",
    "    \n",
    "    return vision_input, []\n",
    "\n",
    "# Test generating output from a sample\n",
    "def generate_text_from_sample(model, processor, sample):\n",
    "    text = processor.apply_chat_template(sample[:2], tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, _ = process_vision_info(sample)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    \n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "    generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "    \n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    \n",
    "    return output_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the base model's performance on a sample\n",
    "if len(train_dataset) > 0:\n",
    "    base_output = generate_text_from_sample(model, processor, train_dataset[0])\n",
    "    print(\"Sample user prompt:\", train_dataset[0][1][\"content\"][1][\"text\"])\n",
    "    print(\"Expected response:\", train_dataset[0][2][\"content\"][0][\"text\"])\n",
    "    print(\"Base model output:\", base_output)\n",
    "\n",
    "# Memory cleanup\n",
    "import gc\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        allocated = torch.cuda.memory_allocated() / (1024 ** 3)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024 ** 3)\n",
    "        print(f\"GPU allocated memory: {allocated:.2f} GB\")\n",
    "        print(f\"GPU reserved memory: {reserved:.2f} GB\")\n",
    "        \n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare for Fine-Tuning\n",
    "\n",
    "Now we'll set up QLoRA fine-tuning configuration to efficiently train the model with limited compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model again, but with quantization for training\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "\n",
    "# BitsAndBytes configuration for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load quantized model and processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "processor = Qwen2VLProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up QLoRA configuration\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Define LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Replace with your own HF username and model name\n",
    "output_dir = \"qwen2-vl-s3-finetuned\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=output_dir,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=output_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Data Collator\n",
    "\n",
    "We need a custom data collator to properly process the text and image pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data collator to encode text and image pairs\n",
    "def collate_fn(examples):\n",
    "    # Get the texts and images, and apply the chat template\n",
    "    texts = [\n",
    "        processor.apply_chat_template(example, tokenize=False) for example in examples\n",
    "    ]\n",
    "    image_inputs = [process_vision_info(example)[0] for example in examples]\n",
    "\n",
    "    # Tokenize the texts and process the images\n",
    "    batch = processor(\n",
    "        text=texts, images=image_inputs, return_tensors=\"pt\", padding=True\n",
    "    )\n",
    "\n",
    "    # The labels are the input_ids, and we mask the padding tokens in the loss computation\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "    # Ignore the image token index in the loss computation (model specific)\n",
    "    if isinstance(processor, Qwen2VLProcessor):\n",
    "        image_tokens = [151652, 151653, 151655]  # Specific image token IDs for Qwen2VLProcessor\n",
    "    else:\n",
    "        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]\n",
    "\n",
    "    # Mask image token IDs in the labels\n",
    "    for image_token_id in image_tokens:\n",
    "        labels[labels == image_token_id] = -100\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Set Up SFT Trainer and Start Training\n",
    "\n",
    "Now we'll set up the SFT Trainer and start the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate the Fine-tuned Model\n",
    "\n",
    "Now let's test our fine-tuned model on some examples to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "clear_memory()\n",
    "\n",
    "# Load base model again\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "processor = Qwen2VLProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Load the fine-tuned adapter\n",
    "adapter_path = training_args.output_dir\n",
    "model.load_adapter(adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a training example\n",
    "if len(train_dataset) > 0:\n",
    "    print(\"Sample from training data:\")\n",
    "    print(\"User prompt:\", train_dataset[0][1][\"content\"][1][\"text\"])\n",
    "    print(\"Expected response:\", train_dataset[0][2][\"content\"][0][\"text\"])\n",
    "    output = generate_text_from_sample(model, processor, train_dataset[0])\n",
    "    print(\"Fine-tuned model output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on an evaluation example\n",
    "if len(eval_dataset) > 0:\n",
    "    print(\"\\nSample from evaluation data:\")\n",
    "    print(\"User prompt:\", eval_dataset[0][1][\"content\"][1][\"text\"])\n",
    "    print(\"Expected response:\", eval_dataset[0][2][\"content\"][0][\"text\"])\n",
    "    output = generate_text_from_sample(model, processor, eval_dataset[0])\n",
    "    print(\"Fine-tuned model output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare with Base Model + Prompting\n",
    "\n",
    "Let's also compare the fine-tuned model with the original model using just prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "clear_memory()\n",
    "\n",
    "# Load the base model without adapter\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "processor = Qwen2VLProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the base model with the same example, but with the system message included\n",
    "if len(train_dataset) > 0:\n",
    "    text = processor.apply_chat_template(train_dataset[0][:2], tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, _ = process_vision_info(train_dataset[0])\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "    generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    print(\"Example user prompt:\", train_dataset[0][1][\"content\"][1][\"text\"])\n",
    "    print(\"Expected response:\", train_dataset[0][2][\"content\"][0][\"text\"])\n",
    "    print(\"Base model with system message output:\", output_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to fine-tune the Qwen2-VL-7B Vision Language Model using TRL on an S3-hosted image-text dataset. We've seen how the model performs before and after fine-tuning, and compared it with a simple prompting approach.\n",
    "\n",
    "Key steps included:\n",
    "1. Loading image-text pairs from an S3 bucket\n",
    "2. Processing and formatting the data for the Qwen VLM model\n",
    "3. Quantizing the model with QLoRA for efficient fine-tuning\n",
    "4. Training the model using the SFT Trainer from TRL\n",
    "5. Evaluating the fine-tuned model's performance\n",
    "\n",
    "The fine-tuned model should now be better at understanding and generating descriptions for images similar to those in your S3 dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 